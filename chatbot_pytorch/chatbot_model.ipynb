{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'airline_intent_dataset.csv',\n",
       " 'Bot.ipynb',\n",
       " 'bot.pt',\n",
       " 'cb_dataset_cleaned - Sheet1.csv',\n",
       " 'chatbot_1_model.ipynb',\n",
       " 'data',\n",
       " 'drop_dataset',\n",
       " 'drop_dataset.zip',\n",
       " 'embed.npy',\n",
       " 'embed.pt',\n",
       " 'flight_a.txt',\n",
       " 'flight_data.ipynb',\n",
       " 'flight_q.txt',\n",
       " 'gg.ipynb',\n",
       " 'good_one.json',\n",
       " 'home',\n",
       " 'intent.txt',\n",
       " 'intent_int.txt',\n",
       " 'int_to_vocab.txt',\n",
       " 'opencon_dataset_augmented2.csv',\n",
       " 'open_a.txt',\n",
       " 'Open_con_Dataset.ipynb',\n",
       " 'open_q.txt',\n",
       " 'preprocessing_chat_bot.ipynb',\n",
       " 'preprocessing_chat_bot.ipynb - Shortcut.lnk',\n",
       " 'preprocessing_chat_bot.py',\n",
       " 'sentrence.txt',\n",
       " 'sentrence_int.txt',\n",
       " 'skipgram.pth',\n",
       " 'skipgram_embed_intent_bot.pt',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'utils.py',\n",
       " 'vocab_to_int.txt',\n",
       " 'weight.pt',\n",
       " 'workspace-1607731597.tar.gz',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('opencon_dataset_augmented2.csv') \n",
    "data=csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=[]\n",
    "answer=[]\n",
    "for i in data:\n",
    "    question.append(i[0])\n",
    "    answer.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot',\n",
       " 'GQ.bot']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is there a bot chatting to me?',\n",
       " 'Is it automated message?',\n",
       " 'Computer based pely',\n",
       " 'Bot or human?',\n",
       " 'Bot is chatting with me?',\n",
       " 'Are you system generated message?',\n",
       " 'Are you robot?',\n",
       " 'Are you machine?',\n",
       " 'Are you just computer?',\n",
       " 'Are you a robot?',\n",
       " 'Are you a person?',\n",
       " 'Are you a machine?',\n",
       " 'Are you a chatbot?',\n",
       " 'Are you a bot?',\n",
       " 'Are these automated messages?',\n",
       " 'Am I talking to a bot?',\n",
       " 'You are a boy are girl?',\n",
       " 'You are a bot or human?',\n",
       " 'This is a machine?',\n",
       " 'This is a chatbot?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAQ.accom',\n",
       " 'SQ.event_prize',\n",
       " 'SQ.event_speakers',\n",
       " 'GQ.help',\n",
       " 'GQ.bot',\n",
       " 'GQ.name',\n",
       " 'FAQ.contact_info',\n",
       " 'JOIN.sponsor',\n",
       " 'SQ.event_details',\n",
       " 'SQ.reg_fee',\n",
       " 'FAQ.food',\n",
       " 'SQ.reg_lastdate',\n",
       " 'FAQ.why_reg',\n",
       " 'SQ.event_schedule',\n",
       " 'JOIN.speaker',\n",
       " 'GQ.gen',\n",
       " 'SQ.IEEE',\n",
       " 'GQ.query',\n",
       " 'SQ.event_date']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent=list(set(answer))\n",
    "intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_int={}\n",
    "int_intent={}\n",
    "for i,j in enumerate(intent):\n",
    "    intent_int.update({j:i})\n",
    "for i,j in intent_int.items():\n",
    "    int_intent.update({i:j})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproces_1(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "\n",
    "def preproces_2(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 0]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'it', 'automated', 'message', '<QUESTION_MARK>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproces_2(question[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_p=[]\n",
    "for i in question:\n",
    "    sent=' '.join(preproces_2(i))+\"\\n\"\n",
    "    question_p.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for i in question_p:\n",
    "    for j in i.split():\n",
    "        words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interested',\n",
       " 'covered',\n",
       " 'below',\n",
       " 'keep',\n",
       " 'un',\n",
       " '2016',\n",
       " 'number',\n",
       " 'march',\n",
       " 'somehow',\n",
       " 'article',\n",
       " 'named',\n",
       " 'placed',\n",
       " 'let',\n",
       " 'dress',\n",
       " 'host',\n",
       " 'intended',\n",
       " 'annual',\n",
       " '2020',\n",
       " 'planned',\n",
       " 'he',\n",
       " 'domains',\n",
       " 'financial',\n",
       " 'themselves',\n",
       " 'provide',\n",
       " 'forum',\n",
       " 'runner',\n",
       " 'supposedly',\n",
       " 'junior',\n",
       " 'upcoming',\n",
       " 'personal',\n",
       " \"what'he\",\n",
       " 'clearly',\n",
       " 'prior',\n",
       " 'cutting',\n",
       " 'legally',\n",
       " 'wall',\n",
       " 'backed',\n",
       " 'notice',\n",
       " 'formally',\n",
       " 'held',\n",
       " 'again',\n",
       " 'wide',\n",
       " 'wondered',\n",
       " 'lose',\n",
       " 'beyond',\n",
       " 'finals',\n",
       " 'approach',\n",
       " '12',\n",
       " 'okay',\n",
       " 'fee',\n",
       " 'unique',\n",
       " 'town',\n",
       " 'others',\n",
       " 'colour',\n",
       " 'restaurant',\n",
       " 'comfort',\n",
       " 'right',\n",
       " 'with',\n",
       " 'besides',\n",
       " 'losing',\n",
       " 'however',\n",
       " 'away',\n",
       " 'adding',\n",
       " 'just',\n",
       " 'numbers',\n",
       " 'reach',\n",
       " 'somewhere',\n",
       " 'please',\n",
       " 'four',\n",
       " 'insurance',\n",
       " 'guests',\n",
       " 'automated',\n",
       " 'special',\n",
       " \"what'las\",\n",
       " 'side',\n",
       " 'an',\n",
       " 'glad',\n",
       " 'to',\n",
       " 'their',\n",
       " 'queries',\n",
       " '7',\n",
       " '1',\n",
       " 'tell',\n",
       " 'something',\n",
       " 'performing',\n",
       " 'successfully',\n",
       " 'using',\n",
       " 'outside',\n",
       " 'organic',\n",
       " 'enjoying',\n",
       " 'remotely',\n",
       " 'mike',\n",
       " 'kitty',\n",
       " 'times',\n",
       " 'deliver',\n",
       " 'sponsored',\n",
       " 'hack',\n",
       " 'that',\n",
       " '14',\n",
       " 'into',\n",
       " 'football',\n",
       " 'martin',\n",
       " 'was',\n",
       " 'end',\n",
       " 'method',\n",
       " 'acting',\n",
       " 'mean',\n",
       " 'idea',\n",
       " 'watching',\n",
       " 'presiding',\n",
       " 'everybody',\n",
       " 'saturday',\n",
       " 'necessarily',\n",
       " 'called',\n",
       " 'your',\n",
       " 'catch',\n",
       " 'minimum',\n",
       " 'arranging',\n",
       " 'subsequently',\n",
       " 'another',\n",
       " 'unfortunate',\n",
       " 'consolation',\n",
       " 'frequently',\n",
       " 'proposed',\n",
       " 'field',\n",
       " 'r',\n",
       " 'team',\n",
       " 'changes',\n",
       " 'happened',\n",
       " 'run',\n",
       " 'timeline',\n",
       " 'advised',\n",
       " 'same',\n",
       " 'similar',\n",
       " \"what'd\",\n",
       " 'fluent',\n",
       " 'trainers',\n",
       " 'additional',\n",
       " 'don',\n",
       " 'design',\n",
       " 'determine',\n",
       " 'goddamn',\n",
       " 'whom',\n",
       " 'ask',\n",
       " 'tour',\n",
       " \"pizza's\",\n",
       " 'avoiding',\n",
       " 'unknown',\n",
       " 'amazing',\n",
       " 'typical',\n",
       " 'deadline',\n",
       " 'absolutely',\n",
       " 'dental',\n",
       " 'way',\n",
       " 'options',\n",
       " 'officially',\n",
       " 'thing',\n",
       " 'acute',\n",
       " 'grand',\n",
       " 'general',\n",
       " 'challenge',\n",
       " 'complete',\n",
       " 'magical',\n",
       " 'rising',\n",
       " 'year',\n",
       " '/',\n",
       " 'bullshit',\n",
       " 'fares',\n",
       " 'clean',\n",
       " 'pleasure',\n",
       " 'holding',\n",
       " 'inaugural',\n",
       " 'youu',\n",
       " 'spreading',\n",
       " 'type',\n",
       " 'issued',\n",
       " 'freaking',\n",
       " 'clarification',\n",
       " 'principal',\n",
       " 'anybody',\n",
       " 'whose',\n",
       " 'value',\n",
       " 'outright',\n",
       " 'organised',\n",
       " 'direct',\n",
       " 'night',\n",
       " 'morning',\n",
       " 'attendees',\n",
       " 'depends',\n",
       " 'concert',\n",
       " 'demanding',\n",
       " 'getting',\n",
       " 'required',\n",
       " 'me',\n",
       " 'locally',\n",
       " 'highest',\n",
       " 'prayer',\n",
       " 'speakers',\n",
       " 'pension',\n",
       " 'die',\n",
       " 'three',\n",
       " 'time',\n",
       " '<PERIOD>',\n",
       " 'fucking',\n",
       " 'my',\n",
       " 'featured',\n",
       " 'and',\n",
       " 'overseeing',\n",
       " 'waiting',\n",
       " 'usually',\n",
       " 'best',\n",
       " 'th',\n",
       " 'wherein',\n",
       " 'initiate',\n",
       " 'his',\n",
       " '<QUESTION_MARK>',\n",
       " 'homecoming',\n",
       " 'fees',\n",
       " 'modern',\n",
       " 'system',\n",
       " 'actual',\n",
       " 'relatives',\n",
       " 'content',\n",
       " 'painting',\n",
       " 'detailed',\n",
       " 'fund',\n",
       " 'plz',\n",
       " 'along',\n",
       " 'under',\n",
       " 'activities',\n",
       " 'ten',\n",
       " 'think',\n",
       " 'section',\n",
       " 'up',\n",
       " 'tongues',\n",
       " 'personally',\n",
       " 'writing',\n",
       " 'kya',\n",
       " 'memorial',\n",
       " 'wait',\n",
       " 'paid',\n",
       " '<EXCLAMATION_MARK>',\n",
       " 'text',\n",
       " 'office',\n",
       " 'visit',\n",
       " 'must',\n",
       " 'are',\n",
       " 'already',\n",
       " 'free',\n",
       " 'campus',\n",
       " 'master',\n",
       " 'crap',\n",
       " 'single',\n",
       " 'attending',\n",
       " 'except',\n",
       " 'itinerary',\n",
       " 'close',\n",
       " 'sunday',\n",
       " 'liquid',\n",
       " 'bear',\n",
       " 'which',\n",
       " 'short',\n",
       " 'stalls',\n",
       " 'jack',\n",
       " 'charities',\n",
       " 'still',\n",
       " 'worth',\n",
       " 'o',\n",
       " 'sure',\n",
       " 'function',\n",
       " 'instantly',\n",
       " 'gain',\n",
       " 'track',\n",
       " 'during',\n",
       " 'get',\n",
       " 'health',\n",
       " 'ourselves',\n",
       " 'procedure',\n",
       " 'organization',\n",
       " 'technology',\n",
       " 'strongly',\n",
       " 'participants',\n",
       " 'potential',\n",
       " 'family',\n",
       " 'observed',\n",
       " 'printed',\n",
       " 'did',\n",
       " 'normal',\n",
       " 'keynote',\n",
       " 'wallet',\n",
       " 'u',\n",
       " 'have',\n",
       " 'willingly',\n",
       " 'extraordinary',\n",
       " 'anything',\n",
       " 'feed',\n",
       " 'half',\n",
       " 'written',\n",
       " 'followed',\n",
       " 'overnight',\n",
       " 'serve',\n",
       " 'hostel',\n",
       " 'prize',\n",
       " 'voter',\n",
       " 'hackathon',\n",
       " 'tonight',\n",
       " 'recruits',\n",
       " 'australia',\n",
       " 'emerge',\n",
       " 'recorded',\n",
       " 'taking',\n",
       " 'far',\n",
       " 'furnished',\n",
       " 'political',\n",
       " 'events',\n",
       " 'hundred',\n",
       " 'certain',\n",
       " 'itself',\n",
       " 'assist',\n",
       " 'said',\n",
       " 'them',\n",
       " 'including',\n",
       " 'separate',\n",
       " 'send',\n",
       " 'event',\n",
       " 'little',\n",
       " 'secretly',\n",
       " 'enable',\n",
       " 'death',\n",
       " 'company',\n",
       " 'out',\n",
       " 'diseases',\n",
       " 'effective',\n",
       " 'because',\n",
       " 'overall',\n",
       " 'interesting',\n",
       " 'such',\n",
       " 'dining',\n",
       " 'halloween',\n",
       " 'involved',\n",
       " \"what'll\",\n",
       " 'straight',\n",
       " 'ing',\n",
       " 'last',\n",
       " 'typically',\n",
       " 'utility',\n",
       " 'myself',\n",
       " 'video',\n",
       " 'organisers',\n",
       " 'length',\n",
       " 'choosing',\n",
       " 'belongs',\n",
       " 'weak',\n",
       " 'quick',\n",
       " 'boat',\n",
       " 'snack',\n",
       " 'directly',\n",
       " 'oral',\n",
       " 'play',\n",
       " 'airport',\n",
       " 'reasons',\n",
       " 'yet',\n",
       " 'arrangement',\n",
       " 'god',\n",
       " 'medical',\n",
       " 'made',\n",
       " 'regards',\n",
       " 'season',\n",
       " 'matching',\n",
       " 'probing',\n",
       " 'specifically',\n",
       " 'name',\n",
       " 'researchers',\n",
       " 'historic',\n",
       " 'those',\n",
       " 'got',\n",
       " 'sponsors',\n",
       " 'properly',\n",
       " 'offered',\n",
       " '6',\n",
       " 'see',\n",
       " 'constantly',\n",
       " 'longer',\n",
       " 'give',\n",
       " 'guide',\n",
       " 'purchase',\n",
       " 'line',\n",
       " 'least',\n",
       " 'farther',\n",
       " 'exchange',\n",
       " 'not',\n",
       " 'create',\n",
       " 'earth',\n",
       " 'elaborate',\n",
       " 'standard',\n",
       " 'assistance',\n",
       " 'plenty',\n",
       " 'mode',\n",
       " 'prominent',\n",
       " 'guess',\n",
       " 'security',\n",
       " 'against',\n",
       " 'honestly',\n",
       " 'originally',\n",
       " 'monthly',\n",
       " 'therefore',\n",
       " 'award',\n",
       " 'hosted',\n",
       " 'by',\n",
       " 'current',\n",
       " 'opencon',\n",
       " 'hurting',\n",
       " 'means',\n",
       " 'group',\n",
       " 'contract',\n",
       " 'accommodations',\n",
       " 'seeing',\n",
       " 'propose',\n",
       " 'gym',\n",
       " 'we',\n",
       " 'private',\n",
       " 'bit',\n",
       " 'loop',\n",
       " 'girl',\n",
       " 'reply',\n",
       " 'competition',\n",
       " 'winning',\n",
       " 'history',\n",
       " 'someone',\n",
       " 'whatas',\n",
       " 'this',\n",
       " 'employment',\n",
       " 'booking',\n",
       " 'two',\n",
       " 'received',\n",
       " 'participating',\n",
       " 'tim',\n",
       " 'playing',\n",
       " 'rice',\n",
       " 'bad',\n",
       " 'meeting',\n",
       " 'christmas',\n",
       " 'beating',\n",
       " 'domains/specializations',\n",
       " 'say',\n",
       " \"what're\",\n",
       " 'eight',\n",
       " 'rich',\n",
       " 'remarkable',\n",
       " 'technological',\n",
       " 'frankly',\n",
       " 'rumors',\n",
       " 'species',\n",
       " 'developer',\n",
       " 'celebrating',\n",
       " 'precisely',\n",
       " 'sir',\n",
       " 'quite',\n",
       " 'student',\n",
       " 'quickly',\n",
       " 'obviously',\n",
       " 'gathering',\n",
       " 'know',\n",
       " 'delivery',\n",
       " 'games',\n",
       " 'cash',\n",
       " 'catering',\n",
       " 'running',\n",
       " 'barely',\n",
       " 'employed',\n",
       " 'wherever',\n",
       " 'gas',\n",
       " 'actually',\n",
       " 'finally',\n",
       " 'birth',\n",
       " 'ya',\n",
       " 'refundable',\n",
       " 'bounty',\n",
       " 'rate',\n",
       " 'german',\n",
       " 'yo',\n",
       " 'ever',\n",
       " 'brief',\n",
       " 'these',\n",
       " 'chatting',\n",
       " 'until',\n",
       " 'orders',\n",
       " 'finding',\n",
       " 'organisation',\n",
       " 'registering',\n",
       " 'website',\n",
       " 'been',\n",
       " 'english',\n",
       " 't-shirts',\n",
       " 'declaring',\n",
       " 'goods',\n",
       " 'winners',\n",
       " 'well',\n",
       " 'box',\n",
       " 'senior',\n",
       " 'fuck',\n",
       " 'vit',\n",
       " 'maybe',\n",
       " 'weekly',\n",
       " 'vacancy',\n",
       " 'washington',\n",
       " 'file',\n",
       " 'back',\n",
       " 'latest',\n",
       " 'accept',\n",
       " 'break',\n",
       " 'tuition',\n",
       " 'live',\n",
       " 'always',\n",
       " 'its',\n",
       " 'accomodation',\n",
       " 'really',\n",
       " 'deficit',\n",
       " 'due',\n",
       " 'extra',\n",
       " 'dare',\n",
       " 'what',\n",
       " 'cancel',\n",
       " 'repeat',\n",
       " 'kinda',\n",
       " 'contribute',\n",
       " 'fields',\n",
       " 'considering',\n",
       " 'terribly',\n",
       " 'shirts',\n",
       " '2015',\n",
       " 'certificate',\n",
       " 'greatly',\n",
       " 'irish',\n",
       " 'speaking',\n",
       " \"it's\",\n",
       " 'attendance',\n",
       " 'argue',\n",
       " 'surely',\n",
       " 'distribution',\n",
       " 'knows',\n",
       " 'of',\n",
       " 'hopefully',\n",
       " 'campaign',\n",
       " 'reference',\n",
       " 'club',\n",
       " 'form',\n",
       " 'discuss',\n",
       " 'johnny',\n",
       " 'between',\n",
       " 'money',\n",
       " 'specializations',\n",
       " 'specific',\n",
       " 'boy',\n",
       " 'hosts',\n",
       " 'schedule',\n",
       " 'offer',\n",
       " 'for',\n",
       " '-',\n",
       " 'graduation',\n",
       " 'list',\n",
       " 'good',\n",
       " 'details',\n",
       " 'everyone',\n",
       " 'behind',\n",
       " 'scheduled',\n",
       " 'externals',\n",
       " 'helping',\n",
       " 'transmission',\n",
       " 'notes',\n",
       " 'passports',\n",
       " 'proper',\n",
       " 'oh',\n",
       " 'never',\n",
       " 'benefit',\n",
       " 'somebody',\n",
       " 'sometimes',\n",
       " 'heaven',\n",
       " 'web',\n",
       " 'scheduling',\n",
       " 'broadcast',\n",
       " 's',\n",
       " 'gonna',\n",
       " 'causing',\n",
       " 'even',\n",
       " 'better',\n",
       " 'thus',\n",
       " 'conducting',\n",
       " 'members',\n",
       " 'account',\n",
       " 'submitted',\n",
       " 'access',\n",
       " 'shall',\n",
       " 'possibly',\n",
       " 'do',\n",
       " 'delivered',\n",
       " 'healthy',\n",
       " 'till',\n",
       " 'accomadated',\n",
       " 'license',\n",
       " 'micro',\n",
       " 'committee',\n",
       " 'life',\n",
       " 'chinese',\n",
       " 'warmly',\n",
       " 'own',\n",
       " 'regarding',\n",
       " 'dog',\n",
       " 'usual',\n",
       " 'stay',\n",
       " 'almost',\n",
       " 'true',\n",
       " 'vegan',\n",
       " 'charge',\n",
       " 'financially',\n",
       " 'admission',\n",
       " 'golf',\n",
       " 'us',\n",
       " 'before',\n",
       " 'starting',\n",
       " 'monitoring',\n",
       " 'willing',\n",
       " 'rewards',\n",
       " 'happening',\n",
       " 'very',\n",
       " 'folks',\n",
       " 'more',\n",
       " 'inspirational',\n",
       " 'added',\n",
       " 'selected',\n",
       " 'at',\n",
       " 'raha',\n",
       " 'literally',\n",
       " 'vital',\n",
       " 'paying',\n",
       " 'famous',\n",
       " 'genretrd',\n",
       " 'especially',\n",
       " 'religious',\n",
       " 'african',\n",
       " 'prospective',\n",
       " 'ac/non',\n",
       " 'knew',\n",
       " 'info',\n",
       " 'ã‚â«',\n",
       " 'corporate',\n",
       " 'hello',\n",
       " 'â€¢',\n",
       " 'describing',\n",
       " 'were',\n",
       " 'indian',\n",
       " 'girls',\n",
       " 'downtown',\n",
       " 'from',\n",
       " 'food/refreshments',\n",
       " 'bizarre',\n",
       " 'news',\n",
       " 'part',\n",
       " 'possible',\n",
       " 'iaâ¢aam',\n",
       " 'korean',\n",
       " 'college',\n",
       " 'based',\n",
       " \"what'â€²\",\n",
       " 'now',\n",
       " 'friend',\n",
       " 'too',\n",
       " 'price',\n",
       " 'voting',\n",
       " 'should',\n",
       " 'diverse',\n",
       " 'five',\n",
       " 'kept',\n",
       " 'bigger',\n",
       " 'question',\n",
       " 'slightly',\n",
       " 'maximum',\n",
       " 'seniors',\n",
       " 'timing',\n",
       " 'meditation',\n",
       " 'training',\n",
       " 'above',\n",
       " 'weekend',\n",
       " 'conversation',\n",
       " 'working',\n",
       " 'gate',\n",
       " 'entire',\n",
       " 'span',\n",
       " 'so',\n",
       " 'whole',\n",
       " 'awaiting',\n",
       " 'denying',\n",
       " \"what'bal\",\n",
       " 'satisfaction',\n",
       " 'behave',\n",
       " 'building',\n",
       " 'hai',\n",
       " 'education',\n",
       " 'also',\n",
       " 'other',\n",
       " 'suggesting',\n",
       " 'compulsory',\n",
       " 'automatically',\n",
       " 'it',\n",
       " 'hi',\n",
       " 'taken',\n",
       " 'rooms',\n",
       " 'since',\n",
       " 'necessary',\n",
       " 'foodie',\n",
       " 'deal',\n",
       " 'cause',\n",
       " 'safe',\n",
       " 'scoring',\n",
       " 'a',\n",
       " 'viva',\n",
       " 'registered',\n",
       " 'explain',\n",
       " 'charitable',\n",
       " 'while',\n",
       " 'whatever',\n",
       " 'our',\n",
       " 'conducted',\n",
       " 'definitely',\n",
       " 'gpay',\n",
       " 'day',\n",
       " '2',\n",
       " 'center',\n",
       " 'non',\n",
       " 'offers',\n",
       " 'achieve',\n",
       " 'initiative',\n",
       " 'later',\n",
       " 'cultural',\n",
       " 'software',\n",
       " 'him',\n",
       " 'once',\n",
       " 'election',\n",
       " 'ki',\n",
       " 'final',\n",
       " 'talk',\n",
       " 'bowl',\n",
       " 'ryan',\n",
       " 'council',\n",
       " 'individual',\n",
       " 're',\n",
       " 'parent',\n",
       " 'gauge',\n",
       " 'winner',\n",
       " 'being',\n",
       " 'people',\n",
       " 'put',\n",
       " \"didn't\",\n",
       " 'reading',\n",
       " 'extent',\n",
       " 'neutral',\n",
       " '2008',\n",
       " 'natural',\n",
       " 'need',\n",
       " 'variety',\n",
       " 'after',\n",
       " 'following',\n",
       " 'i',\n",
       " 'biggest',\n",
       " 'speak',\n",
       " 'inside',\n",
       " \"what'she\",\n",
       " 'community',\n",
       " 'hola',\n",
       " 'dance',\n",
       " 'call',\n",
       " 'search',\n",
       " 'generally',\n",
       " 'particularly',\n",
       " 'database',\n",
       " 'judging',\n",
       " 'kids',\n",
       " 'moved',\n",
       " 'query',\n",
       " 'international',\n",
       " 'there',\n",
       " 'nowhere',\n",
       " 'important',\n",
       " 'chatbot',\n",
       " 'fight',\n",
       " 'odd',\n",
       " 'cost',\n",
       " 'exist',\n",
       " 'basically',\n",
       " 'hows',\n",
       " 'over',\n",
       " 'forthcoming',\n",
       " 'much',\n",
       " 'vogue',\n",
       " 'around',\n",
       " 'lab',\n",
       " 'each',\n",
       " 'course',\n",
       " 'term',\n",
       " 'be',\n",
       " 'v',\n",
       " 'indeed',\n",
       " 'voice',\n",
       " 'remaining',\n",
       " 'ending',\n",
       " 'ride',\n",
       " 'inevitable',\n",
       " 'global',\n",
       " 'else',\n",
       " 'precise',\n",
       " 'permanent',\n",
       " 'accomaodation',\n",
       " 'downstairs',\n",
       " 'answering',\n",
       " 'her',\n",
       " 'needing',\n",
       " 'naming',\n",
       " 'performed',\n",
       " 'timings',\n",
       " 'funds',\n",
       " 'walk',\n",
       " 'or',\n",
       " 'topic',\n",
       " 'guys',\n",
       " 'triggering',\n",
       " 'phone',\n",
       " 'seeking',\n",
       " 'con',\n",
       " 'main',\n",
       " 'further',\n",
       " 'take',\n",
       " 'society',\n",
       " 'joining',\n",
       " 'michael',\n",
       " 'bill',\n",
       " 'alive',\n",
       " 'product',\n",
       " 'advertise',\n",
       " 'incredible',\n",
       " 'than',\n",
       " 'aim',\n",
       " 'win',\n",
       " 'middle',\n",
       " 'yes',\n",
       " 'engineering',\n",
       " 'store',\n",
       " 'table',\n",
       " 'could',\n",
       " '10',\n",
       " 'leave',\n",
       " 'film',\n",
       " 'incredibly',\n",
       " 'make',\n",
       " 'filming',\n",
       " 'address',\n",
       " 'hackathons',\n",
       " 'clear',\n",
       " 'injury',\n",
       " 'eat',\n",
       " 'enough',\n",
       " 'passes',\n",
       " 'vegetarians',\n",
       " 'permission',\n",
       " 'everything',\n",
       " 'like',\n",
       " 'vague',\n",
       " 'can',\n",
       " 'hidden',\n",
       " 'fourth',\n",
       " 'she',\n",
       " 'exists',\n",
       " 'seriously',\n",
       " 'al',\n",
       " 'pm',\n",
       " 'gives',\n",
       " 'outstanding',\n",
       " 'goodies',\n",
       " 'gala',\n",
       " 'pely',\n",
       " 'nature',\n",
       " 'hate',\n",
       " 'hungry',\n",
       " 'machine',\n",
       " 'happen',\n",
       " 'questions',\n",
       " 'makes',\n",
       " 'missing',\n",
       " 'suddenly',\n",
       " 'as',\n",
       " 'travel',\n",
       " 'topping',\n",
       " 'self',\n",
       " 'importance',\n",
       " 'future',\n",
       " 'iam',\n",
       " 'ultimately',\n",
       " 'normally',\n",
       " 'bunk',\n",
       " 'accomadation',\n",
       " 'read',\n",
       " 'x',\n",
       " 'truly',\n",
       " '2013',\n",
       " 'capable',\n",
       " 'movie',\n",
       " 'perhaps',\n",
       " 'internal',\n",
       " 'ieee',\n",
       " 'saying',\n",
       " 'contact',\n",
       " 'fact',\n",
       " 'cannot',\n",
       " 'amongst',\n",
       " 'accommodate',\n",
       " 'unless',\n",
       " 'chronological',\n",
       " 'interest',\n",
       " 'probably',\n",
       " 'daily',\n",
       " 'monetary',\n",
       " \"what'the\",\n",
       " 'sort',\n",
       " 'expected',\n",
       " '2018',\n",
       " 'often',\n",
       " 'correct',\n",
       " 'publicly',\n",
       " 'facebook',\n",
       " 'takes',\n",
       " 'torment',\n",
       " 'successful',\n",
       " 'wanting',\n",
       " 'messages',\n",
       " 'services',\n",
       " 'remains',\n",
       " 'only',\n",
       " 'coming',\n",
       " 'simply',\n",
       " 'punished',\n",
       " 'story',\n",
       " 'technically',\n",
       " 'terrible',\n",
       " 'vegetarian',\n",
       " 'nonsense',\n",
       " 'politely',\n",
       " 'does',\n",
       " 'left',\n",
       " 'welcome',\n",
       " 'larger',\n",
       " 'menu',\n",
       " 'abir',\n",
       " 'slot',\n",
       " 'easy',\n",
       " 'on',\n",
       " 'remember',\n",
       " 'public',\n",
       " 'putting',\n",
       " 'large',\n",
       " 'eventually',\n",
       " 'ought',\n",
       " 'feelings',\n",
       " 'finance',\n",
       " 'favorite',\n",
       " 'likely',\n",
       " 'consortium',\n",
       " 'maria',\n",
       " 'relations',\n",
       " 'either',\n",
       " 'significantly',\n",
       " 'fine',\n",
       " 'research',\n",
       " 'point',\n",
       " 'easily',\n",
       " 'although',\n",
       " 'living',\n",
       " 'click',\n",
       " 'among',\n",
       " 'chosen',\n",
       " 'days',\n",
       " 'random',\n",
       " 'recommend',\n",
       " 'become',\n",
       " 'conference',\n",
       " 'requirement',\n",
       " 'controlling',\n",
       " 'who',\n",
       " 'book',\n",
       " 'big',\n",
       " 'battle',\n",
       " 'canada',\n",
       " 'similarly',\n",
       " 'letter',\n",
       " 'different',\n",
       " 'hot',\n",
       " 'third',\n",
       " 'suggest',\n",
       " 'rental',\n",
       " 'creating',\n",
       " 'shooting',\n",
       " 'allowed',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1410"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is there a bot chatting to me <QUESTION_MARK>\\n',\n",
       " 'is it automated message <QUESTION_MARK>\\n',\n",
       " 'computer based pely\\n',\n",
       " 'bot or human <QUESTION_MARK>\\n',\n",
       " 'bot is chatting with me <QUESTION_MARK>\\n',\n",
       " 'are you system generated message <QUESTION_MARK>\\n',\n",
       " 'are you robot <QUESTION_MARK>\\n',\n",
       " 'are you machine <QUESTION_MARK>\\n',\n",
       " 'are you just computer <QUESTION_MARK>\\n',\n",
       " 'are you a robot <QUESTION_MARK>\\n',\n",
       " 'are you a person <QUESTION_MARK>\\n',\n",
       " 'are you a machine <QUESTION_MARK>\\n',\n",
       " 'are you a chatbot <QUESTION_MARK>\\n',\n",
       " 'are you a bot <QUESTION_MARK>\\n',\n",
       " 'are these automated messages <QUESTION_MARK>\\n',\n",
       " 'am i talking to a bot <QUESTION_MARK>\\n',\n",
       " 'you are a boy are girl <QUESTION_MARK>\\n',\n",
       " 'you are a bot or human <QUESTION_MARK>\\n',\n",
       " 'this is a machine <QUESTION_MARK>\\n',\n",
       " 'this is a chatbot <QUESTION_MARK>\\n']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_p[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(set(words))\n",
    "int_to_words={}\n",
    "words_to_int={}\n",
    "for i,j in enumerate(words,1):\n",
    "    int_to_words.update({i:j})\n",
    "for i,j in int_to_words.items():\n",
    "    words_to_int.update({j:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int=[]\n",
    "for i in question_p:\n",
    "    question_int=[]\n",
    "    for j in i.split():\n",
    "        if j in words:\n",
    "            question_int.append(words_to_int[j])\n",
    "    if len(question_int):    \n",
    "        questions_int.append(question_int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1187, 781, 715, 1184, 485, 78, 194, 216],\n",
       " [1187, 704, 72, 1169, 216],\n",
       " [1209, 663, 879],\n",
       " [1184, 822, 1213, 216],\n",
       " [1184, 1187, 485, 58, 194, 216],\n",
       " [247, 1391, 220, 1073, 1169, 216],\n",
       " [247, 1391, 1339, 216],\n",
       " [247, 1391, 883, 216],\n",
       " [247, 1391, 64, 1209, 216],\n",
       " [247, 1391, 715, 1339, 216],\n",
       " [247, 1391, 715, 1031, 216],\n",
       " [247, 1391, 715, 883, 216],\n",
       " [247, 1391, 715, 784, 216],\n",
       " [247, 1391, 715, 1184, 216],\n",
       " [247, 484, 72, 933, 216],\n",
       " [1222, 763, 1136, 78, 715, 1184, 216],\n",
       " [1391, 247, 715, 554, 247, 424, 216],\n",
       " [1391, 247, 715, 1184, 822, 1213, 216],\n",
       " [431, 1187, 715, 883, 216],\n",
       " [431, 1187, 715, 784, 216],\n",
       " [220, 1073, 425],\n",
       " [457, 1391, 220, 637],\n",
       " [1187, 431, 1209, 663, 1035],\n",
       " [1187, 431, 715, 883, 216],\n",
       " [247, 1391, 715, 1213, 216],\n",
       " [759, 1165, 68],\n",
       " [759, 1165],\n",
       " [763, 759, 1125, 644],\n",
       " [1160, 1391, 1165, 194, 216],\n",
       " [522, 809, 867, 1391, 595, 216],\n",
       " [522, 595, 1391, 595, 216],\n",
       " [522, 867, 1391, 1165, 194, 58, 216],\n",
       " [522, 867, 1391, 595, 216],\n",
       " [522, 247, 1391, 562, 631, 216],\n",
       " [759, 1165, 227],\n",
       " [1207, 867, 1391, 1165, 194],\n",
       " [867, 1391, 1165, 194, 216],\n",
       " [1207, 867, 1391, 319, 216],\n",
       " [1207, 867, 1391, 1165, 194, 216],\n",
       " [1207, 595, 1391, 1165, 216],\n",
       " [1207, 78, 1165, 1391, 216],\n",
       " [1207, 1160, 1391, 319, 194, 216],\n",
       " [763, 759, 1165],\n",
       " [867, 1391, 83, 1125, 644],\n",
       " [1165, 194, 68],\n",
       " [1406],\n",
       " [705],\n",
       " [1406, 242],\n",
       " [1406, 781],\n",
       " [769, 242],\n",
       " [647, 1330],\n",
       " [790, 603, 216],\n",
       " [1207, 103, 115, 726, 216],\n",
       " [1207, 1391, 1068, 216],\n",
       " [1167, 234, 216],\n",
       " [1159, 624, 216],\n",
       " [238, 1181, 632, 216],\n",
       " [1010, 216],\n",
       " [1207, 1391, 1206, 216],\n",
       " [247, 1391, 970, 216],\n",
       " [247, 1391, 562, 216],\n",
       " [1207, 247, 1391, 216],\n",
       " [1207, 247, 1391, 1206, 216],\n",
       " [986, 247, 173, 216],\n",
       " [522, 867, 763, 771, 1391, 216],\n",
       " [115, 372, 216],\n",
       " [867, 1391, 83, 194, 115, 372, 216],\n",
       " [247, 1391, 781, 216],\n",
       " [1401, 1076, 781, 216],\n",
       " [522, 1187, 115, 372, 216],\n",
       " [247, 1391, 951, 216],\n",
       " [1187, 1191, 781, 216],\n",
       " [115, 372, 1187, 216],\n",
       " [986, 1187, 485, 216],\n",
       " [83, 194, 1292, 115, 892, 216],\n",
       " [986, 247, 1391, 216],\n",
       " [986, 1222, 763, 485, 58, 216],\n",
       " [1159, 115, 372, 216],\n",
       " [522, 670, 763, 445, 889, 115, 372, 216],\n",
       " [83, 194, 115, 372, 216],\n",
       " [522, 247, 1391, 114, 216],\n",
       " [763, 292, 715, 675],\n",
       " [759, 78, 1265, 779],\n",
       " [763, 292, 715, 675, 558, 1391],\n",
       " [1024, 715, 779],\n",
       " [763, 292, 715, 779, 558, 1391],\n",
       " [763, 292, 715, 779],\n",
       " [292, 715, 779, 558, 1391],\n",
       " [986, 1187, 588, 431, 325],\n",
       " [522, 946, 1069, 595, 216],\n",
       " [522, 1187, 1069, 216],\n",
       " [848, 1391, 83, 194, 627, 1292, 1069, 216],\n",
       " [1187, 431, 1069, 216],\n",
       " [848, 1391, 83, 194, 627, 522, 946, 908, 595, 216],\n",
       " [522, 1187, 431, 1320, 595, 216],\n",
       " [522, 1187, 908, 216],\n",
       " [522, 1187, 908, 1347, 502],\n",
       " [867, 1391, 384, 1105, 1292, 1069],\n",
       " [522, 1187, 1069, 1402, 558, 216],\n",
       " [522, 908, 730, 216],\n",
       " [1159, 1313, 546, 1292, 216],\n",
       " [522, 946, 1069, 946],\n",
       " [522, 1187, 1069, 216],\n",
       " [522, 1187, 908],\n",
       " [522, 1187, 908, 502, 216],\n",
       " [522, 1108, 946, 908, 1030, 78, 359, 216],\n",
       " [1159, 908],\n",
       " [522, 1187, 1313, 841, 542, 115, 1320, 216],\n",
       " [1207, 287, 908, 1228, 100, 359, 216],\n",
       " [1207, 793, 1124, 704, 787, 78, 1214, 216],\n",
       " [1187, 781, 715, 1231, 50, 216],\n",
       " [1187, 1313, 325, 249, 216],\n",
       " [1207, 793, 1160, 704, 787, 194, 78, 1246, 216],\n",
       " [522, 247, 1313, 1231, 218, 216],\n",
       " [1207, 793, 595, 420, 292, 78, 1178],\n",
       " [522, 1187, 1313, 1231, 50, 542, 1313, 325, 216],\n",
       " [522, 1187, 1313, 218, 193, 78, 1246, 558, 1313, 325, 216],\n",
       " [1144, 1313, 668, 558, 192, 715, 1231, 1305, 1347, 1313, 325],\n",
       " [1207, 793, 946, 704, 787, 216],\n",
       " [1207, 793, 670, 763, 1178, 558, 1231, 216],\n",
       " [1207, 793, 1187, 1313, 1231, 614, 216],\n",
       " [1207, 793, 595, 763, 292, 78, 1178, 78, 1246, 216],\n",
       " [1207, 793, 1187, 1313, 1231, 50],\n",
       " [522, 1187, 1313, 1231, 50, 216],\n",
       " [1187, 781, 715, 1279, 50, 216],\n",
       " [670, 763, 1311, 205, 290, 216],\n",
       " [522, 1187, 1313, 1231, 218, 216],\n",
       " [522, 1124, 800, 1313, 1231, 218, 558, 431, 325, 216],\n",
       " [1207, 793, 536, 583, 787, 216],\n",
       " [1159, 1313, 1231, 218, 216],\n",
       " [522, 1187, 1313, 787, 558, 1313, 325, 216],\n",
       " [1207, 793, 595, 763, 1178, 78, 1246, 216],\n",
       " [867, 763, 1178, 1355, 216],\n",
       " [867, 763, 1178, 954, 1313, 1074, 216],\n",
       " [1160, 1391, 510, 725, 216],\n",
       " [1187, 1187, 477, 216],\n",
       " [1159, 1313, 398, 542, 1376, 216],\n",
       " [1207, 793, 946, 704, 787, 216],\n",
       " [522, 1187, 1313, 1231, 787, 216],\n",
       " [1369, 1187, 1313, 344, 726, 763, 867, 1150],\n",
       " [598, 1369, 867, 763, 1257, 216],\n",
       " [598, 1369, 867, 763, 1246, 216],\n",
       " [1369, 1187, 1313, 151, 78, 1246, 558, 411, 216],\n",
       " [598, 1369, 867, 763, 1246, 216],\n",
       " [1369, 1187, 1313, 151, 558, 1231, 216],\n",
       " [522, 1187, 1313, 344, 1226, 558, 1231],\n",
       " [409, 522, 1226, 670, 420, 1246, 558, 1313, 325, 216],\n",
       " [598, 1369, 867, 763, 1114, 1313, 547],\n",
       " [1369, 1187, 1313, 1231, 805, 216],\n",
       " [1369, 1187, 1313, 344, 1226, 216],\n",
       " [1369, 946, 1313, 1231, 257, 216],\n",
       " [1369, 867, 763, 1246, 598, 216],\n",
       " [1369, 867, 763, 1246, 409, 216],\n",
       " [1369, 1187, 1313, 344, 1226, 542, 1231],\n",
       " [1369, 1187, 1313, 344, 1226, 558, 1231, 216],\n",
       " [344, 1226, 78, 1246],\n",
       " [1144, 1313, 151, 558, 205, 1231, 216],\n",
       " [486, 1369, 867, 763, 1246, 216],\n",
       " [522, 1124, 800, 1313, 344, 1226, 78, 1246, 216],\n",
       " [598, 261, 1226, 867, 763, 1246],\n",
       " [1369, 1187, 1313, 344, 558, 1231, 216],\n",
       " [1369, 1187, 1313, 344, 1226, 78, 1246, 216],\n",
       " [522, 1187, 1313, 151, 558, 1231, 216],\n",
       " [522, 1187, 1313, 344, 726, 558, 1231, 216],\n",
       " [1369, 1187, 1313, 344, 1226, 216],\n",
       " [522, 1187, 1313, 344, 1226, 558, 1231, 216],\n",
       " [598, 522, 1226, 867, 763, 1257, 558, 1313, 325, 216],\n",
       " [1369, 1187, 1313, 344, 726, 78, 1246, 216],\n",
       " [867, 420, 1246, 954, 1074, 216],\n",
       " [522, 1187, 1313, 344, 1226, 558, 1376, 216],\n",
       " [867, 420, 47, 1191, 558, 1231, 1022, 1313, 344, 1226, 861, 216],\n",
       " [1369, 1187, 1313, 325],\n",
       " [522, 1282, 247, 1313, 315, 1063, 78, 800, 954, 216],\n",
       " [1369, 1187, 704, 750, 40, 216],\n",
       " [1369, 1160, 411, 800, 40, 216],\n",
       " [261, 726, 78, 261, 726, 1187, 1313, 325, 216],\n",
       " [1369, 1187, 1313, 325, 216],\n",
       " [1369, 1187, 1313, 325, 1063, 78, 884, 216],\n",
       " [1369, 1187, 1313, 325, 624],\n",
       " [522, 247, 1313, 1282, 542, 1313, 325, 216],\n",
       " [1369, 1187, 1313, 325, 19, 78, 831, 1220],\n",
       " [1369, 207, 1062, 1187, 704, 750, 40, 216],\n",
       " [1369, 1187, 704, 566, 216],\n",
       " [1369, 1187, 431, 325, 216, 595, 420, 292, 78, 1294, 1092, 1313, 979, 216],\n",
       " [1369, 1187, 704],\n",
       " [1369, 1187, 704, 216],\n",
       " [1369, 1187, 1313, 325, 184],\n",
       " [1369, 1187, 431, 325, 216],\n",
       " [1226, 216],\n",
       " [1369, 595, 1391, 759, 205, 1081, 216],\n",
       " [1369, 1160, 98, 325, 831, 1220, 216],\n",
       " [522, 247, 1313, 1282, 1369, 1313, 325, 1187, 40, 216],\n",
       " [954, 261, 1282, 1160, 1313, 325, 800, 40, 216],\n",
       " [522, 1187, 1313, 202, 688, 558, 1313, 325, 216],\n",
       " [522, 1187, 1313, 556, 558, 1313, 325, 216],\n",
       " [1369, 946, 1313, 585, 1061, 207, 104, 216],\n",
       " [522, 247, 1313, 1282, 558, 1313, 325, 216],\n",
       " [954, 522, 1282, 1160, 1313, 325, 800, 723, 216],\n",
       " [1207, 1334, 979, 1313, 325, 1124, 130, 558, 216],\n",
       " [954, 522, 979, 420, 292, 1313, 325, 216],\n",
       " [986, 1160, 800, 311, 1313, 1377],\n",
       " [261, 527, 247, 750, 2, 216],\n",
       " [986, 247, 1313, 198, 1347, 431, 325, 216],\n",
       " [986, 247, 1313, 198, 558, 1313, 325, 216],\n",
       " [986, 1160, 800, 535, 631, 1313, 325, 216],\n",
       " [986, 247, 1313, 198, 216],\n",
       " [986, 1187, 937, 78, 1313, 983],\n",
       " [986,\n",
       "  247,\n",
       "  1313,\n",
       "  198,\n",
       "  558,\n",
       "  1313,\n",
       "  325,\n",
       "  207,\n",
       "  522,\n",
       "  823,\n",
       "  1124,\n",
       "  1065,\n",
       "  800,\n",
       "  535,\n",
       "  1292,\n",
       "  216],\n",
       " [986, 1092, 1160, 800, 937, 78, 384, 1377],\n",
       " [986, 1160, 800, 1313, 1183, 216],\n",
       " [986, 1187, 1063, 78, 855, 1313, 462, 216],\n",
       " [986, 1092, 247, 1313, 198, 1347, 431, 325, 216],\n",
       " [986, 1187, 535],\n",
       " [986, 247, 1313, 198, 242],\n",
       " [986, 247, 1313, 71],\n",
       " [986, 247, 1313, 198, 216],\n",
       " [198, 216],\n",
       " [522, 1292, 1313, 137, 216],\n",
       " [595, 1391, 292, 1401, 198, 937, 1347, 216],\n",
       " [986, 1124, 800, 110, 889, 198, 558, 1313, 1302, 216],\n",
       " [986, 247, 1313, 198],\n",
       " [986, 1092, 247, 1313, 198, 558, 1313, 305, 216],\n",
       " [848, 1391, 372, 1313, 198, 986, 1160, 800, 937, 216],\n",
       " [986, 247, 1313, 198, 216],\n",
       " [1207, 1334, 198, 1160, 781, 800, 216],\n",
       " [1160, 431, 1183, 800, 781, 216],\n",
       " [867, 763, 381, 715, 561, 542, 198, 216],\n",
       " [160, 644, 1292, 1313, 198, 216],\n",
       " [1369, 1160, 1313, 198, 800, 937, 216],\n",
       " [867, 420, 741, 78, 321, 236, 216],\n",
       " [522, 444, 247, 1065, 583, 741, 1292, 216],\n",
       " [522, 1160, 1313, 749, 275],\n",
       " [247, 781, 466, 1317, 216],\n",
       " [946, 1313, 749, 275, 715, 303, 216],\n",
       " [1317, 558, 1313, 497, 216],\n",
       " [595, 1313, 751, 986, 753, 842, 1243, 818, 498, 275, 295, 216],\n",
       " [247, 781, 1401, 1317, 216],\n",
       " [1187, 781, 715, 749, 303, 216],\n",
       " [522, 1187, 1313, 303, 558, 1313, 749, 542, 1313, 97],\n",
       " [247, 781, 1401, 1317, 558, 497, 216],\n",
       " [522, 1160, 763, 275, 1022, 763, 842, 1313, 411],\n",
       " [522, 1187, 1313, 1248, 216],\n",
       " [522, 247, 1313, 623, 216],\n",
       " [522, 1187, 1313, 466, 303, 216],\n",
       " [522, 1160, 763, 275, 216],\n",
       " [522, 595, 763, 275, 1022, 763, 842, 216],\n",
       " [1207, 793, 1187, 1313, 303, 551],\n",
       " [522, 1187, 1313, 303, 551, 216],\n",
       " [407, 216],\n",
       " [1160, 763, 275, 1125, 478, 216],\n",
       " [595, 1391, 292, 1401, 1317, 216],\n",
       " [522, 1292, 1313, 668, 551, 216],\n",
       " [522, 867, 763, 275, 216],\n",
       " [1159, 1313, 303, 551, 216],\n",
       " [595, 420, 275, 1401, 1317, 216],\n",
       " [522, 247, 1313, 1317, 750, 379, 216],\n",
       " [522, 1187, 1313, 303, 551, 216],\n",
       " [522, 1187, 1313, 1143, 558, 192, 715, 303, 216],\n",
       " [522, 1187, 1313, 1032, 558, 1313, 1317, 216],\n",
       " [1160, 781, 800, 877, 216],\n",
       " [522, 247, 1313, 1317, 558, 497, 216],\n",
       " [595, 26, 1314, 275, 84, 216],\n",
       " [1160, 420, 275, 1034, 558, 814, 885, 1347, 325, 216],\n",
       " [1160, 420, 275, 494, 1347, 1313, 325, 216],\n",
       " [1160, 1313, 303, 800, 466, 822, 877, 822, 700, 1089, 216],\n",
       " [1207, 1160, 1313, 303, 800, 1275, 216],\n",
       " [1159, 1187, 1313, 303, 216],\n",
       " [247, 781, 1033, 207, 995, 1273, 667, 216],\n",
       " [522, 1187, 1313, 131, 542, 1313, 325],\n",
       " [522, 1187, 1313, 556, 216],\n",
       " [522, 1187, 1313, 1229, 542, 1313, 325, 216],\n",
       " [867, 763, 381, 1313, 325, 556, 216],\n",
       " [1144, 1313, 556, 216],\n",
       " [522, 1187, 1313, 556, 558, 1313, 325, 216],\n",
       " [522, 1187, 679, 542, 1313, 325],\n",
       " [1207, 1124, 1313, 325, 1308, 954, 216],\n",
       " [522, 1187, 1313, 256, 542, 115, 325],\n",
       " [522, 1187, 1313, 556],\n",
       " [1369, 1187, 1313, 325, 566, 216],\n",
       " [1369, 1187, 1313, 325, 216, 867, 420, 850, 1347, 550, 216],\n",
       " [867, 763, 292, 1313, 256, 216],\n",
       " [1159, 1313, 556],\n",
       " [522, 1187, 1313, 256],\n",
       " [325, 256, 216],\n",
       " [556],\n",
       " [1144, 1313, 202, 847, 558, 431, 325, 216],\n",
       " [522, 1187, 1313, 1229, 542, 315, 216],\n",
       " [1369, 1187, 704, 1063, 78, 800, 40, 216],\n",
       " [1369, 1187, 704, 583, 1061],\n",
       " [1159, 1313, 325, 819, 207, 556, 216],\n",
       " [522, 1187, 1313, 225, 325, 556],\n",
       " [1207, 1187, 1313, 325, 556, 216],\n",
       " [1369, 247, 1313, 315, 624, 216],\n",
       " [522, 1187, 1313, 256, 216],\n",
       " [1369, 1187, 1313, 325, 750, 408, 216],\n",
       " [595, 420, 292, 78, 610, 781, 1313, 690, 726, 216],\n",
       " [1207, 1334, 979, 1313, 325, 1124, 1308, 558, 216],\n",
       " [1369, 1187, 1313, 983, 216],\n",
       " [522, 1160, 884, 1369, 216],\n",
       " [522, 1187, 431, 1292, 216],\n",
       " [522, 1187, 431, 325, 1292, 216],\n",
       " [522, 1187, 1311, 828, 216],\n",
       " [522, 1187, 411, 216],\n",
       " [522, 1187, 431, 216],\n",
       " [522, 1187, 431, 325, 1292, 216],\n",
       " [522, 1194, 1187, 1311, 828],\n",
       " [789, 1187, 704, 715, 983, 822, 305, 216],\n",
       " [522, 1187, 411, 789, 1292, 203, 203, 216],\n",
       " [522, 1187, 1313, 325, 1092, 1292, 216],\n",
       " [522, 1194, 1187, 431, 325, 216],\n",
       " [1159, 1313, 325, 216],\n",
       " [522, 1187, 431],\n",
       " [522, 1187, 431, 1292],\n",
       " [522, 1187, 431, 325, 1292],\n",
       " [522, 1187, 431, 1292, 216],\n",
       " [522, 1187, 1313, 325, 1292, 216],\n",
       " [1144, 689, 1380, 1347, 431, 325, 216],\n",
       " [522, 946, 704, 1116],\n",
       " [522, 1187, 1313, 325, 1092, 1292, 216],\n",
       " [522, 1187, 431, 325, 1292],\n",
       " [522, 1187, 1313, 325, 1292, 216],\n",
       " [522, 1187, 431, 1092, 1292, 216],\n",
       " [522, 1160, 420, 595, 1347, 431, 325, 216],\n",
       " [522, 1089, 595, 420, 275, 654, 431, 325, 216],\n",
       " [1207, 1187, 704, 993, 654, 700, 856, 216],\n",
       " [1062, 867, 763, 610],\n",
       " [1160, 1313, 1340, 800, 1210, 1347, 1231, 218, 216],\n",
       " [1062, 867, 763, 610, 216],\n",
       " [522, 1292, 516, 558, 1313, 282, 216],\n",
       " [1187, 1340, 1384, 216],\n",
       " [1187, 899, 1188, 558, 567, 216],\n",
       " [522, 1160, 420, 595, 558, 1340],\n",
       " [1124, 763, 800, 1384, 1340, 409, 502, 216],\n",
       " [1187, 610, 1384, 558, 618],\n",
       " [1207, 1187, 1313, 516, 1305, 216],\n",
       " [1207, 1160, 1391, 914, 216],\n",
       " [1187, 1340, 249, 216],\n",
       " [1062, 867, 763, 610, 216],\n",
       " [1062, 867, 763, 610, 216],\n",
       " [522, 1292, 722, 610],\n",
       " [522, 1187, 1313, 1340, 1019, 216],\n",
       " [1062, 595, 763, 610, 216],\n",
       " [1062, 1160, 763, 610, 216],\n",
       " [522, 247, 1313, 416, 1384, 216],\n",
       " [1160, 781, 800, 1401, 1340, 558, 1313, 282, 216],\n",
       " [1187, 1340, 1384],\n",
       " [1160, 291, 24, 1340, 274, 1313, 325, 216],\n",
       " [595, 1391, 24, 1340, 216],\n",
       " [1160, 1340, 800, 1291, 216],\n",
       " [1207, 1160, 420, 800, 599, 274, 1313, 325, 216],\n",
       " [867, 763, 610, 88, 250, 216],\n",
       " [247, 781, 1401, 416, 1347, 1313, 1205, 216],\n",
       " [867, 763, 610, 58, 715, 666, 1347, 1085, 302, 216],\n",
       " [1092, 1313, 563, 542, 1313, 1340, 203],\n",
       " [1124, 1340, 800, 249, 822, 241, 216],\n",
       " [1124, 1340, 800, 1384, 1347, 715, 1119, 216],\n",
       " [1124, 704, 800, 642, 1237, 216],\n",
       " [867, 763, 513, 88, 1313, 502, 250, 216],\n",
       " [1160, 420, 1384, 707, 822, 1300, 216],\n",
       " [1160, 1269, 800, 1384],\n",
       " [522, 1187, 1313, 950, 216],\n",
       " [595, 420, 275, 1071, 274, 1313, 325, 216, 522, 1071, 216],\n",
       " [522, 1292, 655, 274, 1313, 325, 216],\n",
       " [595, 420, 292, 78, 275, 722, 606, 1071, 216],\n",
       " [1160, 1071, 800, 1384, 216],\n",
       " [1160, 1071, 800, 1384, 216],\n",
       " [522, 247, 212, 1127, 558, 1071],\n",
       " [1124, 1071, 800, 1384, 216],\n",
       " [1160, 420, 800, 1384, 1071, 274, 1313, 325],\n",
       " [1160, 1071, 800, 1384],\n",
       " [1160, 1313, 1071, 800, 1384, 216],\n",
       " [1187, 781, 1401, 572, 1071, 363, 216],\n",
       " [522, 1292, 1351, 216],\n",
       " [1160, 1071, 800, 1384, 216],\n",
       " [247, 420, 583, 800, 1384, 1071],\n",
       " [522, 247, 1313, 1142, 379, 216],\n",
       " [1187, 781, 1071, 216],\n",
       " [763,\n",
       "  1222,\n",
       "  715,\n",
       "  710,\n",
       "  242,\n",
       "  1160,\n",
       "  763,\n",
       "  275,\n",
       "  1125,\n",
       "  562,\n",
       "  1071,\n",
       "  1369,\n",
       "  763,\n",
       "  1222,\n",
       "  882,\n",
       "  216],\n",
       " [1160, 1391, 296, 618, 216],\n",
       " [1160, 1071, 800, 1384, 216],\n",
       " [1187, 1071, 1210],\n",
       " [1160, 781, 800, 249, 1071, 216],\n",
       " [522, 247, 1313, 1127, 558, 1071, 216],\n",
       " [522, 1071, 1127, 593, 800, 781, 216],\n",
       " [522, 1292, 1269, 216],\n",
       " [1187, 781, 1078, 1071, 216],\n",
       " [1160, 781, 800, 613, 1071, 216],\n",
       " [522, 1160, 800, 1384, 78, 618, 889, 1071, 78, 800, 810, 242],\n",
       " [595, 420, 275, 1313, 1071, 934, 1092, 1313, 979, 542, 1313, 325, 216],\n",
       " [595, 420, 292, 78, 1178, 1353, 558, 1071, 216],\n",
       " [1160, 781, 800, 263, 216],\n",
       " [1124, 1071, 1127, 800, 249, 542, 787, 216],\n",
       " [1124, 146, 800, 1384, 216],\n",
       " [247, 781, 1127, 558, 862, 216],\n",
       " [247, 781, 1127, 558, 729, 862, 216],\n",
       " [763, 292, 830, 1141, 763, 1006, 78, 857],\n",
       " [986, 78, 910, 558, 830, 1141, 216],\n",
       " [384, 194, 1313, 910, 644, 542, 429, 654, 908],\n",
       " [986, 670, 763, 910, 558, 627, 644, 216],\n",
       " [143, 867, 763, 910, 216],\n",
       " [986, 595, 763, 910, 607, 431, 325, 216],\n",
       " [986, 670, 763, 910],\n",
       " [143, 78, 910, 216],\n",
       " [143, 867, 763, 910, 1347, 1031, 654, 1236],\n",
       " [143, 78, 910, 558, 627, 1105],\n",
       " [1207, 78, 910, 1313, 489, 602, 216],\n",
       " [143, 867, 420, 910, 558, 80, 216],\n",
       " [867, 763, 275, 1125, 65],\n",
       " [763, 759, 627, 1165],\n",
       " [986, 670, 763, 741, 78],\n",
       " [910, 644, 216],\n",
       " [826],\n",
       " [763,\n",
       "  1006,\n",
       "  627,\n",
       "  563,\n",
       "  1292,\n",
       "  1313,\n",
       "  325,\n",
       "  595,\n",
       "  1391,\n",
       "  463,\n",
       "  429,\n",
       "  986,\n",
       "  848,\n",
       "  1165,\n",
       "  194,\n",
       "  331,\n",
       "  216],\n",
       " [986, 867, 763, 910, 558, 830, 563, 216],\n",
       " [143, 595, 763, 759, 78, 910, 216],\n",
       " [986, 867, 763, 910, 558, 627, 563],\n",
       " [143, 670, 763, 910, 216],\n",
       " [143, 867, 763, 910, 558, 80, 216],\n",
       " [848, 1391, 24, 1313, 563, 542, 715, 1031, 78, 910, 216],\n",
       " [986, 867, 763, 910, 558, 80, 607, 1313, 325, 216],\n",
       " [986, 867, 763, 771, 558, 178, 216],\n",
       " [1207, 867, 763, 66, 1313, 325, 555, 216],\n",
       " [910, 563, 542, 349],\n",
       " [986, 867, 763, 910, 558, 80, 607, 812, 216],\n",
       " [1187,\n",
       "  781,\n",
       "  1401,\n",
       "  160,\n",
       "  1251,\n",
       "  1095,\n",
       "  822,\n",
       "  1190,\n",
       "  1062,\n",
       "  420,\n",
       "  867,\n",
       "  324,\n",
       "  722,\n",
       "  885,\n",
       "  216],\n",
       " [986, 670, 763, 910, 558, 1340, 1127, 216],\n",
       " [763, 292, 1125, 160, 779, 986, 670, 763, 741, 78, 216],\n",
       " [522, 1160, 763, 275, 331, 542, 431],\n",
       " [1321, 670, 763, 1274, 216],\n",
       " [522, 1187, 689, 73, 1347, 115, 325, 98, 763, 670, 1246, 1347, 704, 216],\n",
       " [1321, 670, 763, 1246, 558, 411, 216],\n",
       " [522, 1160, 763, 275, 331, 542, 431, 325, 216],\n",
       " [1321, 670, 763, 1246, 216],\n",
       " [1321, 670, 763, 1246, 558, 431, 325, 216],\n",
       " [522, 1160, 763, 272, 409, 833, 431, 325, 216],\n",
       " [522, 247, 1313, 1089, 558, 194, 78, 1246, 1347, 1313, 325, 216],\n",
       " [1321, 670, 763, 1363, 411, 791, 700, 315],\n",
       " [1207, 1187, 704, 1319, 715, 1165, 954, 750, 717, 216],\n",
       " [522, 1187, 1313, 575, 542, 254, 431, 1302, 216],\n",
       " [1321, 670, 420, 1246, 216],\n",
       " [522, 946, 431, 595, 558, 194],\n",
       " [1159, 689, 73, 1292, 431, 325, 216],\n",
       " [1321, 670, 763, 1246, 558, 431, 325],\n",
       " [1321, 670, 763, 1274, 78, 431, 325, 216],\n",
       " [781, 247, 1334, 315, 1321, 431, 325, 216],\n",
       " [1160, 431, 325, 267, 290, 216],\n",
       " [522, 1187, 336, 1292, 431, 216],\n",
       " [522, 1089, 1124, 763, 275, 409, 436, 216],\n",
       " [522, 126, 420, 583, 275, 654, 431],\n",
       " [1321, 670, 763, 1246, 558, 1397, 325, 216],\n",
       " [1321, 670, 420, 1246, 558, 1313, 325, 216],\n",
       " [1321, 670, 763, 1246, 216],\n",
       " [1321, 670, 420, 1246, 558, 1313, 325, 216],\n",
       " [522, 1187, 73, 1292, 1313, 325, 216],\n",
       " [1321, 431, 325, 207, 391, 1125, 700, 216],\n",
       " [1321, 670, 763, 1274, 558, 115, 325, 216],\n",
       " [1159, 689, 73, 1292, 431, 1174, 325, 216],\n",
       " [522, 595, 420, 731, 409, 1313, 104, 542, 431, 325, 216],\n",
       " [1207, 867, 763, 1165, 1313, 325],\n",
       " [595, 1391, 292, 860, 1023, 216],\n",
       " [763, 1006, 78, 1189, 431, 325],\n",
       " [763, 1124, 865, 78, 1189, 411, 18, 203, 986, 670, 763, 910, 216],\n",
       " [763, 1124, 865, 78, 1189, 1391, 203],\n",
       " [1207, 867, 429, 800, 715, 1189, 216],\n",
       " [763, 1, 1347, 1271, 1313, 325],\n",
       " [1321, 670, 763, 226, 115, 325, 216],\n",
       " [1187, 115, 962, 1305, 275, 508, 78, 618, 1306, 1022, 1391, 759, 295],\n",
       " [1207, 78, 1189, 1313, 325, 216],\n",
       " [1207, 78, 1189, 1313, 325, 216],\n",
       " [522, 247, 1391, 622, 78, 1189, 216],\n",
       " [867, 1391, 838, 558, 194, 216],\n",
       " [867, 715, 1189, 1150, 216],\n",
       " [1207, 867, 763, 1189, 431, 325],\n",
       " [1006, 78, 1189, 115, 325],\n",
       " [763, 1006, 78, 1189],\n",
       " [1112, 622, 78, 526, 551, 558, 431, 325, 203],\n",
       " [1207, 595, 763, 66, 558, 1401, 283, 1215, 216],\n",
       " [1062, 867, 763, 1246, 889, 377, 216],\n",
       " [420, 1124, 865, 78, 1189, 115, 325],\n",
       " [867, 420, 1257, 558, 1215, 216],\n",
       " [763, 1124, 865, 78, 1150, 1391, 889, 715, 1189, 203],\n",
       " [522, 1022, 715, 1189, 1407, 78, 1150, 618, 216],\n",
       " [595, 1391, 759, 627, 820, 216],\n",
       " [867, 722, 330, 800, 715, 658, 542, 431, 1327, 889, 715, 1189, 216],\n",
       " [1124, 1391, 865, 78, 548, 627, 1215, 216],\n",
       " [867, 420, 910, 1391, 558, 894, 1215, 216],\n",
       " [522, 547, 542, 1165, 247, 1391, 827, 1347, 1215, 216],\n",
       " [522, 247, 1313, 1089, 1347, 1271, 115, 325, 216],\n",
       " [143, 78, 910, 558, 1215, 1091, 216],\n",
       " [1401, 894, 1089, 558, 1023, 115, 325, 216],\n",
       " [867, 763, 982, 715, 1183],\n",
       " [1187, 781, 1401, 505, 558, 627, 198, 216],\n",
       " [763, 1124, 865, 78, 765, 1347, 115, 325],\n",
       " [763, 1124, 865, 78, 800, 1148, 889, 715, 1183, 558, 411, 203],\n",
       " [143, 78, 910, 1022, 763, 1006, 78, 1274, 889, 715, 1183, 216],\n",
       " [867, 763, 765, 631, 115, 325, 216],\n",
       " [1207, 867, 429, 800, 715, 1183, 216],\n",
       " [763, 1124, 865, 78, 95, 715, 1102, 1347, 1313, 983],\n",
       " [1187, 781, 715, 1220, 558, 1183, 1347, 115, 325],\n",
       " [1207, 78, 800, 715, 658, 542, 1313, 732, 216],\n",
       " [1207, 78, 800, 715, 658, 542, 1313, 325, 216],\n",
       " [522, 247, 1391, 622, 78, 765, 1292, 216],\n",
       " [867, 763, 765, 631, 115, 325, 216],\n",
       " [867, 715, 1183, 1150, 216],\n",
       " [1207, 867, 763, 1150, 431, 325, 889, 715, 1183],\n",
       " [763, 1006, 78, 800, 715, 1183, 631, 115, 325],\n",
       " [763, 1006, 78, 765],\n",
       " [763, 1222, 622, 78, 384, 715, 1102, 558, 431, 325, 203],\n",
       " [522, 1187, 1313, 278, 78, 982, 715, 1183, 216],\n",
       " [1187, 781, 1401, 952, 1188, 558, 198, 216],\n",
       " [1187, 781, 1401, 1111, 558, 115, 1102, 216],\n",
       " [867, 763, 1274, 889, 715, 1183, 216],\n",
       " [1124, 1290, 78, 250, 800, 1384, 216],\n",
       " [763, 1124, 865, 78, 1150, 1391, 889, 715, 1183, 203],\n",
       " [522, 1022, 715, 1183, 1407, 78, 1150, 618, 216],\n",
       " [1207, 595, 763, 1257, 78, 800, 715, 658, 542, 1313, 1286, 216],\n",
       " [522, 527, 595, 1391, 759, 198, 654, 216],\n",
       " [522, 1187, 1313, 1143, 193, 558, 1313, 198, 216],\n",
       " [1187, 781, 715, 316, 1184, 485, 465, 78, 194, 216],\n",
       " [1187, 1313, 704, 72, 1169, 216],\n",
       " [1209, 1075, 663, 879],\n",
       " [968, 1184, 822, 1213, 216],\n",
       " [1184, 1187, 485, 41, 58, 194, 216],\n",
       " [247, 1391, 220, 1073, 715, 1169, 216],\n",
       " [247, 1391, 1391, 1339, 216],\n",
       " [247, 1391, 1391, 883, 216],\n",
       " [247, 1391, 438, 64, 1209, 216],\n",
       " [247, 98, 1391, 715, 1339, 216],\n",
       " [247, 867, 1391, 715, 1031, 216],\n",
       " [1243, 247, 1391, 715, 883, 216],\n",
       " [247, 1391, 1137, 715, 784, 216],\n",
       " [247, 484, 1391, 715, 1184, 216],\n",
       " [1321, 247, 484, 72, 933, 216],\n",
       " [1222, 763, 473, 1136, 78, 431, 715, 1184, 216],\n",
       " [1321, 1391, 247, 1243, 715, 554, 247, 424, 216],\n",
       " [334, 1391, 247, 715, 1096, 1184, 822, 1213, 216],\n",
       " [431, 224, 1187, 715, 883, 216],\n",
       " [431, 1187, 715, 612, 784, 216],\n",
       " [1313, 220, 1073, 425],\n",
       " [457, 1391, 292, 220, 637],\n",
       " [1187, 431, 1174, 1209, 663, 1035],\n",
       " [1187, 431, 715, 1086, 883, 216],\n",
       " [247, 1391, 715, 612, 1213, 216],\n",
       " [759, 1125, 1165, 68],\n",
       " [704, 759, 1165],\n",
       " [287, 763, 759, 1125, 644],\n",
       " [1409, 1160, 1391, 1165, 194, 216],\n",
       " [522, 809, 362, 867, 1391, 595, 216],\n",
       " [1409, 522, 595, 1391, 595, 216],\n",
       " [522, 867, 763, 1391, 1165, 194, 711, 58, 216],\n",
       " [1243, 522, 867, 1391, 595, 216],\n",
       " [573, 522, 247, 1391, 562, 631, 216],\n",
       " [291, 759, 1165, 227],\n",
       " [1207, 867, 1391, 517, 1165, 194],\n",
       " [867, 1391, 1165, 1211, 194, 216],\n",
       " [1207, 867, 1065, 1391, 319, 216],\n",
       " [1207, 867, 1391, 266, 1165, 194, 216],\n",
       " [665, 1207, 595, 1391, 1165, 216],\n",
       " [1207, 78, 517, 1165, 1391, 216],\n",
       " [1207, 1194, 1160, 1391, 319, 194, 216],\n",
       " [763, 759, 815, 1165],\n",
       " [867, 1391, 83, 194, 1125, 644],\n",
       " [1165, 116, 194, 68],\n",
       " [264, 1406],\n",
       " [966, 705],\n",
       " [320, 1406, 242],\n",
       " [437, 1406, 781],\n",
       " [1110, 769, 242],\n",
       " [381, 647, 1330],\n",
       " [699, 790, 603, 216],\n",
       " [1207, 103, 115, 1007, 726, 216],\n",
       " [1207, 746, 1391, 1068, 216],\n",
       " [689, 1167, 234, 216],\n",
       " [446, 582, 624, 216],\n",
       " [1370, 238, 1181, 1256, 632, 216],\n",
       " [20, 1010, 216],\n",
       " [1207, 1391, 266, 1206, 216],\n",
       " [1207, 247, 1391, 970, 216],\n",
       " [247, 375, 1391, 562, 216],\n",
       " [715, 1207, 247, 1391, 216],\n",
       " [1207, 1239, 247, 1391, 1206, 216],\n",
       " [986, 393, 247, 173, 216],\n",
       " [522, 867, 763, 68, 771, 1391, 216],\n",
       " [1187, 115, 372, 216],\n",
       " [986, 867, 1391, 83, 78, 194, 115, 372, 216],\n",
       " [1391, 247, 1391, 781, 216],\n",
       " [1401, 344, 1076, 781, 216],\n",
       " [522, 1187, 229, 115, 372, 216],\n",
       " [247, 420, 1391, 951, 216],\n",
       " [1187, 870, 1191, 781, 216],\n",
       " [115, 372, 344, 1187, 216],\n",
       " [986, 517, 1187, 485, 216],\n",
       " [83, 194, 1292, 203, 203, 203, 115, 892, 216],\n",
       " [986, 809, 247, 1391, 216],\n",
       " [986, 586, 1222, 763, 485, 58, 216],\n",
       " [1159, 517, 115, 372, 216],\n",
       " [522, 670, 1222, 763, 445, 889, 115, 493, 372, 216],\n",
       " [83, 194, 115, 1033, 372, 216],\n",
       " [522, 247, 704, 1391, 114, 216],\n",
       " [763, 514, 292, 715, 675],\n",
       " [420, 759, 78, 1265, 779],\n",
       " [763, 292, 715, 675, 1278, 558, 1391],\n",
       " [1024, 715, 253, 779],\n",
       " [763, 292, 1125, 715, 779, 558, 1391],\n",
       " [763, 292, 366, 715, 779],\n",
       " [292, 715, 779, 1059, 558, 1391],\n",
       " [400, 986, 1187, 588, 431, 325],\n",
       " [503, 522, 946, 431, 908, 559, 502, 595, 216],\n",
       " [699, 522, 1187, 908, 559, 849, 502, 216],\n",
       " [848, 1391, 83, 64, 194, 1125, 627, 1292, 908, 559, 380, 502, 216],\n",
       " [522, 1187, 431, 76, 908, 559, 502, 216],\n",
       " [848, 1391, 68, 83, 194, 627, 64, 522, 946, 908, 431, 595, 216],\n",
       " [522, 1098, 1187, 431, 1320, 595, 216],\n",
       " [522, 279, 1187, 908, 216],\n",
       " [522, 1187, 908, 1206, 1347, 502],\n",
       " [1207, 867, 595, 1391, 384, 1105, 1292, 908, 559, 502],\n",
       " [522, 346, 1187, 908, 559, 99, 502, 1402, 558, 216],\n",
       " [522, 908, 965, 730, 216],\n",
       " [689, 1159, 468, 1313, 546, 1292, 216],\n",
       " [207, 522, 946, 392, 908, 559, 502, 946],\n",
       " [1249, 522, 1187, 908, 559, 553, 502, 216],\n",
       " [207, 522, 1187, 908],\n",
       " [207, 522, 1187, 908, 502, 216],\n",
       " [689, 522, 1108, 946, 908, 1320, 1030, 78, 359, 216],\n",
       " [1159, 627, 908],\n",
       " [98, 522, 1187, 1313, 841, 665, 542, 115, 1320, 216],\n",
       " [381, 1207, 287, 98, 908, 1228, 100, 359, 216],\n",
       " [1207, 793, 1008, 1124, 704, 292, 787, 78, 1214, 216],\n",
       " [207, 1187, 781, 715, 1231, 50, 216],\n",
       " [1187, 1313, 757, 325, 249, 216],\n",
       " [1207, 793, 1160, 704, 633, 787, 194, 665, 78, 1246, 216],\n",
       " [1243, 522, 247, 1313, 1231, 218, 216],\n",
       " [1207, 1093, 793, 595, 420, 514, 292, 78, 1178],\n",
       " [1243, 522, 809, 1187, 1313, 1231, 50, 542, 1313, 325, 216],\n",
       " [522, 1187, 1313, 512, 218, 193, 1306, 78, 357, 1246, 558, 1313, 325, 216],\n",
       " [430, 413, 1313, 668, 972, 558, 192, 1347, 715, 1231, 1305, 1347, 1313, 325],\n",
       " [689, 1207, 793, 946, 704, 787, 216],\n",
       " [665, 1207, 793, 670, 763, 1178, 558, 1080, 1231, 216],\n",
       " [1207, 793, 1187, 1347, 1313, 1231, 600, 614, 216],\n",
       " [1207, 1380, 793, 595, 763, 585, 292, 78, 1178, 1132, 78, 1246, 216],\n",
       " [1207, 793, 551, 1187, 1313, 1231, 50],\n",
       " [522, 1187, 1313, 1080, 1231, 50, 216],\n",
       " [1187, 473, 781, 715, 1232, 1279, 50, 216],\n",
       " [670, 763, 1311, 558, 205, 290, 216],\n",
       " [522, 1187, 1313, 410, 1231, 218, 216],\n",
       " [522, 1124, 689, 800, 1313, 1231, 218, 558, 1206, 431, 17, 325, 216],\n",
       " [689, 1207, 793, 536, 98, 583, 787, 216],\n",
       " [693, 582, 1313, 740, 1231, 218, 216],\n",
       " [522, 972, 1187, 1313, 787, 558, 1313, 1138, 325, 216],\n",
       " [1207, 793, 793, 38, 595, 763, 1178, 78, 1246, 216],\n",
       " [867, 763, 1274, 1178, 1355, 216],\n",
       " [867, 763, 1178, 954, 202, 1313, 997, 1074, 216],\n",
       " [1160, 1391, 391, 510, 725, 216],\n",
       " [1187, 1315, 1187, 477, 216],\n",
       " [665, 1159, 1313, 398, 203, 203, 203, 542, 1376, 216],\n",
       " [1207, 793, 946, 237, 704, 787, 216],\n",
       " [522, 389, 1187, 1313, 1231, 787, 216],\n",
       " [1243, 1369, 1187, 1313, 344, 142, 726, 763, 867, 1150],\n",
       " [598, 1238, 1369, 867, 763, 1257, 216],\n",
       " [1409, 598, 1369, 867, 763, 1246, 216],\n",
       " [1369, 1187, 1313, 151, 78, 39, 1246, 735, 558, 410, 411, 216],\n",
       " [598, 1369, 867, 763, 1308, 1246, 216],\n",
       " [1369, 1187, 1313, 923, 151, 558, 459, 1231, 216],\n",
       " [522, 1187, 1313, 1128, 344, 1226, 978, 558, 1231],\n",
       " [409, 522, 1366, 1226, 670, 420, 1246, 347, 558, 1313, 1192, 325, 216],\n",
       " [598, 665, 1369, 867, 763, 474, 1114, 1313, 547],\n",
       " [1369, 1187, 203, 203, 203, 1313, 1231, 805, 216],\n",
       " [1369, 619, 1187, 1313, 344, 1226, 216],\n",
       " [1369, 486, 946, 1313, 1231, 257, 216],\n",
       " [1369, 867, 763, 1246, 431, 598, 216],\n",
       " [1369, 867, 1165, 763, 1246, 409, 216],\n",
       " [1369, 1187, 1313, 1231, 344, 333, 1226, 542, 1231],\n",
       " [1369, 1187, 8, 1313, 1208, 344, 1226, 558, 1231, 216],\n",
       " [344, 310, 1226, 78, 1246],\n",
       " [822, 430, 1313, 151, 558, 205, 662, 1231, 216],\n",
       " [391, 486, 1369, 867, 763, 1246, 216],\n",
       " [522, 1076, 1124, 98, 800, 1313, 344, 1226, 78, 1246, 216],\n",
       " [598, 456, 261, 1226, 867, 763, 1246],\n",
       " [689, 1369, 1187, 1313, 561, 344, 558, 1231, 216],\n",
       " [1369, 1187, 1238, 1313, 344, 1193, 1226, 78, 1246, 216],\n",
       " [522, 1187, 665, 1313, 151, 558, 579, 1231, 216],\n",
       " [64, 522, 1187, 954, 1313, 344, 726, 558, 1231, 216],\n",
       " [1369, 1187, 431, 1313, 344, 1226, 216],\n",
       " [522, 1187, 1313, 344, 1231, 1226, 234, 558, 1231, 216],\n",
       " [1409, 598, 522, 1226, 867, 763, 1257, 1391, 558, 1347, 1313, 325, 216],\n",
       " [1369, 1369, 1187, 1313, 344, 726, 620, 78, 1246, 216],\n",
       " [867, 420, 1246, 954, 79, 1074, 216],\n",
       " [1243, 522, 1187, 631, 1313, 344, 1226, 558, 1376, 216],\n",
       " [936,\n",
       "  867,\n",
       "  420,\n",
       "  47,\n",
       "  1191,\n",
       "  558,\n",
       "  1231,\n",
       "  1398,\n",
       "  1022,\n",
       "  1313,\n",
       "  1128,\n",
       "  344,\n",
       "  1226,\n",
       "  861,\n",
       "  216],\n",
       " [689, 1369, 1187, 1313, 325],\n",
       " [954, 522, 1282, 247, 558, 1313, 315, 1063, 1064, 78, 800, 954, 216],\n",
       " [1369, 1187, 344, 704, 750, 40, 216],\n",
       " [64, 1369, 809, 1160, 411, 800, 40, 216],\n",
       " [261, 410, 726, 78, 261, 894, 726, 1187, 1313, 325, 216],\n",
       " [708, 1369, 1187, 1313, 325, 216],\n",
       " [1369, 1187, 585, 1313, 325, 1063, 78, 585, 884, 216],\n",
       " [1369, 202, 1187, 1313, 325, 624],\n",
       " [522, 247, 1313, 1282, 150, 542, 1313, 325, 325, 216],\n",
       " [1369, 1306, 1187, 1313, 325, 19, 78, 831, 311, 1220],\n",
       " [1369, 542, 207, 1062, 1187, 103, 704, 750, 40, 216],\n",
       " [1369, 78, 1187, 704, 566, 216],\n",
       " [522,\n",
       "  1369,\n",
       "  1187,\n",
       "  431,\n",
       "  325,\n",
       "  216,\n",
       "  595,\n",
       "  420,\n",
       "  64,\n",
       "  292,\n",
       "  78,\n",
       "  382,\n",
       "  1294,\n",
       "  1194,\n",
       "  1092,\n",
       "  1313,\n",
       "  979,\n",
       "  216],\n",
       " [1369, 665, 1187, 704],\n",
       " [1369, 1369, 1187, 704, 216],\n",
       " [1369, 704, 1187, 1313, 325, 184],\n",
       " [1369, 1187, 937, 431, 325, 216],\n",
       " [522, 1226, 216],\n",
       " [1369, 595, 1391, 1132, 759, 1401, 205, 1081, 216],\n",
       " [708, 1369, 1160, 98, 325, 1160, 831, 1220, 216],\n",
       " [654, 522, 247, 1194, 1313, 1282, 1369, 1313, 325, 1187, 344, 40, 216],\n",
       " [954, 261, 1282, 1160, 1313, 410, 325, 800, 733, 40, 216],\n",
       " [522, 1187, 1313, 19, 202, 1053, 688, 558, 1313, 325, 216],\n",
       " [203, 203, 203, 522, 165, 1187, 1313, 556, 558, 1313, 325, 216],\n",
       " [1369, 946, 1313, 585, 1061, 542, 207, 207, 104, 216],\n",
       " [334, 522, 247, 1313, 1282, 1140, 558, 1313, 325, 216],\n",
       " [954, 522, 375, 1282, 1160, 1313, 956, 325, 800, 723, 216],\n",
       " [1207, 1093, 1334, 979, 1313, 325, 1124, 130, 1054, 558, 216],\n",
       " [954, 522, 993, 979, 420, 292, 1313, 133, 325, 216],\n",
       " [986, 1160, 800, 311, 1313, 197, 1377],\n",
       " [261, 527, 247, 750, 1157, 2, 216],\n",
       " [986, 247, 1313, 289, 198, 1210, 1347, 431, 325, 216],\n",
       " [986, 247, 11, 1313, 206, 198, 558, 1313, 325, 216],\n",
       " [400, 986, 1160, 800, 535, 631, 558, 1313, 325, 216],\n",
       " [986, 1092, 247, 1313, 198, 216],\n",
       " [986, 1187, 937, 78, 245, 1313, 983],\n",
       " [986,\n",
       "  247,\n",
       "  1313,\n",
       "  198,\n",
       "  558,\n",
       "  1313,\n",
       "  410,\n",
       "  325,\n",
       "  207,\n",
       "  522,\n",
       "  823,\n",
       "  704,\n",
       "  1124,\n",
       "  247,\n",
       "  1065,\n",
       "  514,\n",
       "  800,\n",
       "  535,\n",
       "  1292,\n",
       "  216],\n",
       " [986, 1092, 1160, 800, 937, 234, 78, 384, 187, 1377],\n",
       " [665, 986, 1160, 800, 1313, 1183, 216],\n",
       " [986, 1194, 1187, 482, 1063, 78, 855, 1313, 462, 216],\n",
       " [689, 986, 1092, 228, 247, 1313, 198, 1347, 431, 325, 216],\n",
       " [381, 986, 1187, 535],\n",
       " [986, 203, 203, 203, 247, 1313, 198, 242],\n",
       " [986, 247, 1313, 410, 71],\n",
       " [986, 247, 1313, 829, 198, 216],\n",
       " [1048, 198, 216],\n",
       " [522, 1292, 1092, 1313, 137, 216],\n",
       " [595, 1391, 292, 1401, 198, 267, 937, 357, 1347, 216],\n",
       " [619, 986, 1124, 800, 110, 889, 936, 198, 558, 104, 1313, 1302, 216],\n",
       " [986, 247, 699, 1313, 198],\n",
       " [986, 1092, 247, 542, 1313, 283, 198, 558, 1313, 1133, 305, 216],\n",
       " [848, 1391, 68, 372, 1313, 198, 986, 1160, 514, 800, 1146, 937, 216],\n",
       " [986, 247, 665, 1313, 198, 216],\n",
       " [822, 1207, 625, 1334, 198, 1160, 781, 800, 216],\n",
       " [1160, 391, 431, 1183, 800, 781, 216],\n",
       " [867, 763, 203, 203, 203, 381, 715, 561, 542, 659, 198, 216],\n",
       " [98, 160, 644, 1292, 1313, 198, 216],\n",
       " [59, 1369, 1160, 1313, 198, 473, 800, 937, 216],\n",
       " [867, 420, 741, 41, 78, 215, 321, 236, 216],\n",
       " [522, 700, 21, 166, 552, 809, 247, 1092, 1065, 583, 741, 1292, 216],\n",
       " [522, 1160, 1153, 1313, 749, 275],\n",
       " [247, 420, 781, 466, 1317, 216],\n",
       " [946, 443, 1313, 749, 275, 715, 466, 303, 216],\n",
       " [1317, 558, 1313, 161, 497, 216],\n",
       " [595,\n",
       "  1313,\n",
       "  288,\n",
       "  751,\n",
       "  468,\n",
       "  986,\n",
       "  503,\n",
       "  753,\n",
       "  842,\n",
       "  1243,\n",
       "  32,\n",
       "  818,\n",
       "  498,\n",
       "  275,\n",
       "  295,\n",
       "  216],\n",
       " [247, 781, 203, 203, 203, 1401, 1317, 216],\n",
       " [1187, 781, 715, 1132, 749, 303, 216],\n",
       " [1153, 522, 1187, 1313, 829, 303, 558, 1313, 749, 233, 542, 1313, 97],\n",
       " [822, 247, 781, 781, 1401, 1317, 558, 497, 216],\n",
       " [522, 1160, 763, 482, 275, 1022, 763, 867, 842, 936, 1313, 411],\n",
       " [906, 522, 1187, 1313, 1248, 216],\n",
       " [938, 522, 247, 1313, 623, 216],\n",
       " [207, 522, 1187, 1313, 466, 303, 216],\n",
       " [522, 1160, 482, 763, 275, 216],\n",
       " [522, 440, 595, 867, 763, 275, 1022, 763, 842, 216],\n",
       " [689, 1207, 793, 1187, 1313, 303, 551],\n",
       " [522, 1187, 558, 1313, 303, 551, 216],\n",
       " [143, 407, 216],\n",
       " [1160, 763, 275, 1125, 627, 478, 216],\n",
       " [595, 1391, 292, 1401, 876, 1317, 216],\n",
       " [522, 1292, 635, 1313, 668, 551, 216],\n",
       " [1409, 522, 867, 763, 275, 216],\n",
       " [1159, 78, 1313, 876, 303, 551, 216],\n",
       " [595, 420, 275, 1401, 627, 1317, 216],\n",
       " [522, 361, 247, 558, 1313, 1317, 750, 379, 216],\n",
       " [522, 1187, 1313, 303, 207, 551, 216],\n",
       " [522, 1187, 1313, 1143, 558, 699, 192, 337, 715, 303, 216],\n",
       " [522, 1187, 1313, 1032, 105, 558, 1092, 1313, 1317, 216],\n",
       " [1160, 781, 514, 800, 877, 216],\n",
       " [522, 247, 1313, 674, 1317, 865, 558, 497, 216],\n",
       " [595, 26, 1314, 275, 558, 84, 216],\n",
       " [1160, 420, 275, 1313, 1034, 558, 814, 1325, 885, 1347, 431, 325, 216],\n",
       " [1160, 420, 266, 275, 1040, 559, 1113, 530, 1347, 1313, 1132, 325, 216],\n",
       " [1160, 1313, 303, 1047, 800, 654, 466, 822, 877, 822, 700, 719, 1089, 216],\n",
       " [1207, 1160, 1313, 303, 959, 800, 689, 1275, 216],\n",
       " [921, 582, 809, 1187, 1313, 303, 216],\n",
       " [247, 420, 781, 1033, 1403, 207, 995, 1273, 667, 216],\n",
       " [522, 1162, 1187, 1313, 131, 542, 1003, 1313, 325],\n",
       " [522, 1187, 1313, 770, 556, 216],\n",
       " [522, 1187, 64, 1313, 1229, 542, 1313, 949, 325, 216],\n",
       " [1062, 867, 763, 381, 1306, 1313, 325, 556, 216],\n",
       " [430, 1313, 1295, 556, 216],\n",
       " [522, 1187, 1130, 1313, 556, 558, 1313, 740, 325, 216],\n",
       " [522, 1187, 679, 542, 427, 1313, 325],\n",
       " [207, 1207, 383, 1124, 1313, 325, 1308, 954, 216],\n",
       " [207, 522, 1187, 1313, 256, 542, 115, 894, 325],\n",
       " [689, 522, 1187, 1313, 556],\n",
       " [1369, 1187, 704, 1313, 325, 566, 216],\n",
       " [1369, 1369, 1187, 431, 1313, 325, 216, 867, 103, 420, 850, 1347, 550, 216],\n",
       " [867, 1391, 763, 292, 1313, 609, 256, 216],\n",
       " [135, 582, 1313, 556],\n",
       " [522, 1187, 1313, 145, 256],\n",
       " [609, 325, 256, 216],\n",
       " [162, 556],\n",
       " [1409, 430, 650, 1313, 202, 847, 558, 431, 325, 216],\n",
       " [522, 893, 1187, 1313, 810, 1229, 542, 315, 216],\n",
       " [64, 1369, 1187, 704, 1063, 78, 800, 927, 40, 216],\n",
       " [1369, 1187, 704, 583, 574, 1061],\n",
       " [203, 203, 203, 1159, 1313, 325, 207, 819, 207, 1061, 556, 216],\n",
       " [522, 946, 1187, 1313, 225, 325, 556],\n",
       " [1207, 1187, 870, 1313, 325, 556, 216],\n",
       " [1369, 809, 247, 1313, 315, 624, 216],\n",
       " [1409, 522, 1187, 58, 1313, 256, 216],\n",
       " [207, 1369, 1306, 1187, 1313, 325, 750, 408, 216],\n",
       " [595, 420, 266, 292, 487, 78, 610, 954, 781, 1313, 690, 726, 216],\n",
       " [1207, 1352, 1334, 659, 979, 1313, 325, 1124, 1308, 558, 216],\n",
       " [1369, 1187, 1313, 780, 983, 216],\n",
       " [522, 1194, 1160, 884, 1369, 216],\n",
       " [522, 1187, 64, 431, 1292, 216],\n",
       " [522, 1187, 431, 690, 325, 1292, 216],\n",
       " [522, 1187, 1311, 1313, 828, 216],\n",
       " [522, 1187, 1354, 411, 216],\n",
       " [1347, 522, 1187, 431, 216],\n",
       " [522, 1187, 431, 431, 325, 1292, 216],\n",
       " [207, 522, 1194, 1187, 1311, 828],\n",
       " [789, 522, 1187, 431, 704, 64, 715, 983, 822, 305, 216],\n",
       " [522, 1194, 1187, 411, 789, 562, 1292, 203, 203, 216],\n",
       " [936, 522, 809, 1187, 1313, 325, 1092, 1292, 216],\n",
       " [203, 203, 203, 522, 1194, 1187, 431, 325, 216],\n",
       " [1159, 1313, 1380, 325, 216],\n",
       " [522, 657, 1187, 431],\n",
       " [522, 202, 1187, 431, 1292],\n",
       " [522, 712, 1187, 431, 325, 1292],\n",
       " [522, 1187, 431, 1092, 1292, 216],\n",
       " [522, 1187, 654, 1313, 325, 1292, 216],\n",
       " [430, 689, 204, 1380, 1347, 1067, 431, 325, 216],\n",
       " [64, 522, 946, 704, 1116],\n",
       " [522, 1187, 1313, 325, 420, 1092, 1283, 1292, 216],\n",
       " [522, 1187, 431, 325, 902, 1292],\n",
       " [522, 1216, 1187, 1313, 325, 1292, 216],\n",
       " [522, 57, 1187, 431, 1092, 1292, 216],\n",
       " [522, 1160, 482, 420, 595, 1347, 1221, 431, 325, 216],\n",
       " [522, 1380, 1089, 595, 420, 275, 62, 654, 431, 325, 216],\n",
       " [1207, 1187, 171, 704, 517, 993, 64, 654, 700, 856, 216],\n",
       " [1062, 867, 1222, 763, 610],\n",
       " [1160, 1313, 1340, 1268, 800, 1210, 41, 1347, 1231, 218, 216],\n",
       " [1062, 867, 347, 763, 610, 216],\n",
       " [207, 522, 1292, 516, 558, 311, 1313, 282, 216],\n",
       " [1187, 1340, 498, 1384, 216],\n",
       " [587, 1187, 899, 1188, 938, 558, 567, 216],\n",
       " [522, 1160, 420, 595, 558, 321, 1340],\n",
       " [1369, 1124, 763, 800, 1384, 138, 1340, 409, 502, 216],\n",
       " [1187, 610, 1384, 514, 558, 618],\n",
       " [631, 1207, 123, 1187, 1313, 516, 1305, 216],\n",
       " [1207, 460, 1160, 1391, 914, 216],\n",
       " [522, 1187, 1340, 249, 216],\n",
       " [1062, 665, 867, 763, 610, 216],\n",
       " [78, 1062, 867, 763, 610, 216],\n",
       " [522, 627, 1292, 722, 610],\n",
       " [522, 1212, 1187, 1313, 1340, 1019, 216],\n",
       " [1062, 595, 763, 959, 610, 216],\n",
       " [1062, 1160, 1036, 763, 610, 216],\n",
       " [498, 522, 247, 1313, 416, 1384, 216],\n",
       " [1160, 959, 781, 800, 1401, 1340, 558, 441, 1313, 282, 216],\n",
       " [1187, 138, 1340, 1384],\n",
       " [1160, 291, 24, 1188, 1340, 274, 1313, 956, 325, 216],\n",
       " [595, 1391, 473, 24, 1340, 216],\n",
       " [1160, 1340, 665, 800, 1291, 216],\n",
       " [1207, 925, 1160, 867, 420, 800, 378, 599, 274, 1313, 325, 216],\n",
       " [867, 203, 203, 203, 763, 610, 88, 250, 216],\n",
       " [699, 247, 781, 1401, 958, 416, 1347, 1313, 1205, 216],\n",
       " [867, 870, 763, 610, 58, 715, 666, 737, 1347, 215, 166, 815, 1046, 302, 216],\n",
       " [1092, 1293, 1313, 563, 542, 1313, 244, 1340, 203],\n",
       " [337, 1124, 1340, 800, 249, 614, 822, 241, 216],\n",
       " [1124, 1340, 802, 800, 925, 1384, 1347, 715, 1119, 216],\n",
       " [522, 1124, 1092, 704, 800, 1237, 166, 729, 1237, 216],\n",
       " [867, 763, 1308, 513, 88, 1313, 653, 502, 250, 216],\n",
       " [1160, 1367, 420, 1384, 707, 813, 822, 1300, 216],\n",
       " [1160, 572, 1269, 800, 1384],\n",
       " [522, 7, 1187, 1313, 950, 216],\n",
       " [595, 1207, 420, 275, 1071, 274, 1313, 690, 325, 216, 384, 522, 1071, 216],\n",
       " [522, 1292, 249, 1071, 166, 890, 1269, 1384, 274, 1313, 325, 216],\n",
       " [595, 420, 292, 78, 275, 1186, 722, 751, 606, 1071, 216],\n",
       " [1160, 1071, 925, 800, 1384, 216],\n",
       " [1369, 1160, 1071, 800, 1384, 216],\n",
       " [522, 247, 212, 1127, 291, 558, 1071],\n",
       " [213, 1124, 1071, 800, 1384, 216],\n",
       " [1160, 800, 420, 800, 1313, 1384, 1071, 274, 1313, 325],\n",
       " [1160, 1071, 800, 1223, 1384],\n",
       " [889, 1160, 1313, 1071, 800, 1384, 216],\n",
       " [98, 1187, 585, 781, 1401, 572, 1071, 363, 216],\n",
       " [522, 1292, 115, 1351, 216],\n",
       " [1243, 1160, 1071, 800, 1384, 216],\n",
       " [247, 476, 420, 583, 800, 1384, 1071],\n",
       " [522, 760, 247, 1313, 1142, 379, 216],\n",
       " [822, 1187, 781, 1071, 216],\n",
       " [763,\n",
       "  1222,\n",
       "  64,\n",
       "  715,\n",
       "  942,\n",
       "  710,\n",
       "  242,\n",
       "  1160,\n",
       "  763,\n",
       "  275,\n",
       "  618,\n",
       "  1125,\n",
       "  562,\n",
       "  604,\n",
       "  1071,\n",
       "  1369,\n",
       "  391,\n",
       "  763,\n",
       "  1222,\n",
       "  882,\n",
       "  216],\n",
       " [1160, 1391, 514, 296, 618, 216],\n",
       " [1160, 1071, 800, 482, 1384, 216],\n",
       " [1062, 1187, 1071, 1210],\n",
       " [1160, 781, 800, 1401, 249, 1071, 216],\n",
       " [822, 522, 247, 1313, 1127, 558, 431, 1071, 216],\n",
       " [522, 1071, 1127, 593, 20, 800, 706, 781, 216],\n",
       " [522, 103, 1292, 1269, 216],\n",
       " [822, 1187, 781, 729, 559, 661, 943, 1071, 216],\n",
       " [1369, 1160, 781, 800, 1401, 613, 1071, 216],\n",
       " [522,\n",
       "  1160,\n",
       "  800,\n",
       "  1356,\n",
       "  1384,\n",
       "  78,\n",
       "  484,\n",
       "  618,\n",
       "  1247,\n",
       "  889,\n",
       "  1071,\n",
       "  78,\n",
       "  800,\n",
       "  810,\n",
       "  242],\n",
       " [595,\n",
       "  595,\n",
       "  420,\n",
       "  275,\n",
       "  1313,\n",
       "  57,\n",
       "  1071,\n",
       "  207,\n",
       "  934,\n",
       "  1092,\n",
       "  1313,\n",
       "  979,\n",
       "  542,\n",
       "  1313,\n",
       "  325,\n",
       "  216],\n",
       " [1207, 595, 420, 665, 292, 78, 1178, 627, 1353, 558, 1071, 216],\n",
       " [1160, 781, 902, 800, 263, 216],\n",
       " [1124, 1071, 1270, 1127, 800, 249, 542, 917, 787, 216],\n",
       " [207, 1124, 1175, 582, 800, 1384, 216],\n",
       " [1207, 247, 781, 1127, 558, 1012, 862, 216],\n",
       " [247, 781, 781, 1127, 558, 729, 640, 862, 216],\n",
       " [763, 266, 292, 830, 1141, 763, 1006, 736, 78, 857],\n",
       " [986, 78, 910, 736, 558, 830, 1213, 1141, 216],\n",
       " [384, 194, 234, 1313, 910, 644, 542, 429, 993, 654, 908],\n",
       " [986, 1409, 670, 763, 771, 910, 558, 627, 644, 216],\n",
       " [143, 1194, 867, 763, 910, 216],\n",
       " [558, 986, 595, 763, 910, 736, 607, 431, 325, 216],\n",
       " [203, 203, 203, 986, 670, 763, 910],\n",
       " [83, 143, 78, 910, 216],\n",
       " [143, 867, 665, 763, 910, 1347, 431, 1031, 654, 631, 1236],\n",
       " [143, 586, 78, 910, 558, 627, 1105],\n",
       " [1207, 78, 214, 910, 1306, 1313, 489, 602, 216],\n",
       " [143, 867, 996, 420, 910, 41, 558, 80, 216],\n",
       " [867, 420, 763, 275, 1125, 65],\n",
       " [763, 759, 627, 558, 1165],\n",
       " [986, 700, 670, 763, 741, 78],\n",
       " [910, 58, 644, 216],\n",
       " [815, 826],\n",
       " [1243,\n",
       "  763,\n",
       "  1006,\n",
       "  585,\n",
       "  627,\n",
       "  30,\n",
       "  563,\n",
       "  1064,\n",
       "  1292,\n",
       "  1313,\n",
       "  325,\n",
       "  595,\n",
       "  1391,\n",
       "  463,\n",
       "  429,\n",
       "  986,\n",
       "  64,\n",
       "  848,\n",
       "  1165,\n",
       "  194,\n",
       "  331,\n",
       "  216],\n",
       " [986, 482, 867, 763, 517, 910, 558, 830, 563, 216],\n",
       " [143, 830, 595, 1065, 763, 759, 78, 910, 216],\n",
       " [986, 867, 763, 910, 1391, 558, 627, 1276, 563],\n",
       " [1243, 143, 670, 763, 910, 216],\n",
       " [143, 867, 763, 910, 665, 558, 627, 80, 216],\n",
       " [207, 848, 1391, 514, 24, 1313, 563, 542, 715, 553, 1031, 78, 910, 216],\n",
       " [986, 986, 867, 763, 910, 558, 1287, 80, 607, 1313, 16, 325, 216],\n",
       " [986, 867, 763, 665, 771, 558, 722, 178, 216],\n",
       " [665, 1207, 867, 763, 66, 100, 1313, 325, 555, 216],\n",
       " [207, 910, 563, 542, 349],\n",
       " [986, 867, 595, 763, 910, 1191, 558, 80, 68, 607, 812, 216],\n",
       " [207,\n",
       "  1187,\n",
       "  391,\n",
       "  781,\n",
       "  1401,\n",
       "  160,\n",
       "  1251,\n",
       "  1095,\n",
       "  822,\n",
       "  1190,\n",
       "  1062,\n",
       "  420,\n",
       "  373,\n",
       "  867,\n",
       "  973,\n",
       "  324,\n",
       "  722,\n",
       "  885,\n",
       "  216],\n",
       " [986, 670, 763, 594, 910, 558, 1340, 276, 1127, 216],\n",
       " [763, 292, 1125, 160, 1193, 779, 1409, 986, 670, 1165, 763, 741, 78, 216],\n",
       " [522, 1160, 232, 763, 517, 275, 331, 542, 431],\n",
       " [1409, 1321, 670, 763, 1274, 216],\n",
       " [822,\n",
       "  522,\n",
       "  1187,\n",
       "  699,\n",
       "  689,\n",
       "  73,\n",
       "  1347,\n",
       "  115,\n",
       "  325,\n",
       "  98,\n",
       "  391,\n",
       "  763,\n",
       "  670,\n",
       "  1246,\n",
       "  704,\n",
       "  1347,\n",
       "  704,\n",
       "  216],\n",
       " [1321, 670, 595, 763, 936, 1246, 558, 411, 216],\n",
       " [822, 522, 1160, 763, 275, 331, 542, 542, 431, 325, 216],\n",
       " [1321, 585, 670, 763, 1246, 216],\n",
       " [1321, 670, 763, 1246, 815, 558, 431, 988, 325, 216],\n",
       " [522, 1160, 763, 482, 272, 409, 888, 833, 431, 325, 216],\n",
       " [522,\n",
       "  247,\n",
       "  1292,\n",
       "  1313,\n",
       "  1089,\n",
       "  558,\n",
       "  1024,\n",
       "  194,\n",
       "  254,\n",
       "  78,\n",
       "  1246,\n",
       "  1347,\n",
       "  1313,\n",
       "  325,\n",
       "  216],\n",
       " [1321, 699, 670, 763, 1363, 411, 791, 1092, 700, 315],\n",
       " [558, 1207, 1187, 704, 1319, 715, 1165, 189, 954, 215, 750, 717, 216],\n",
       " [522, 1187, 1313, 575, 1131, 542, 254, 431, 681, 1302, 216],\n",
       " [1321, 670, 391, 420, 1246, 216],\n",
       " [522, 946, 431, 1220, 595, 558, 194],\n",
       " [865, 1159, 689, 73, 1292, 1067, 431, 325, 216],\n",
       " [1321, 670, 763, 585, 1246, 703, 558, 431, 325],\n",
       " [1321, 670, 763, 1274, 836, 78, 523, 431, 325, 216],\n",
       " [781, 1092, 247, 1334, 51, 315, 1321, 431, 325, 216],\n",
       " [1160, 842, 431, 325, 267, 290, 216],\n",
       " [522, 1187, 336, 203, 203, 203, 1292, 431, 216],\n",
       " [522, 1089, 98, 1124, 763, 275, 78, 409, 436, 216],\n",
       " [522, 562, 126, 420, 583, 275, 654, 873, 431],\n",
       " [1321, 670, 763, 1246, 78, 558, 1048, 1397, 325, 216],\n",
       " [1321, 670, 420, 391, 1246, 564, 558, 1313, 325, 216],\n",
       " [1321, 670, 763, 514, 1246, 216],\n",
       " [1321, 1409, 670, 420, 1246, 558, 1313, 1128, 325, 216],\n",
       " [522, 972, 1187, 73, 1292, 1255, 1313, 325, 216],\n",
       " [1321, 103, 431, 325, 207, 391, 1321, 1125, 700, 216],\n",
       " [1243, 1321, 670, 763, 1274, 558, 431, 115, 325, 216],\n",
       " [865, 1159, 689, 177, 73, 306, 1292, 431, 1174, 325, 216],\n",
       " [522, 809, 595, 420, 731, 64, 409, 1313, 104, 837, 542, 431, 325, 216],\n",
       " [1207, 867, 763, 1165, 58, 1313, 325],\n",
       " [595, 1391, 292, 1093, 860, 1023, 216],\n",
       " [763, 1006, 78, 699, 1189, 431, 325],\n",
       " [763,\n",
       "  236,\n",
       "  1124,\n",
       "  865,\n",
       "  815,\n",
       "  78,\n",
       "  1189,\n",
       "  411,\n",
       "  18,\n",
       "  203,\n",
       "  986,\n",
       "  1409,\n",
       "  670,\n",
       "  763,\n",
       "  733,\n",
       "  910,\n",
       "  216],\n",
       " [763, 1124, 865, 78, 586, 1189, 1125, 1391, 203],\n",
       " [558, 1207, 867, 429, 902, 800, 715, 1189, 216],\n",
       " [763, 1, 1347, 1271, 458, 1313, 325],\n",
       " [1321, 670, 763, 514, 226, 226, 115, 325, 216],\n",
       " [1187,\n",
       "  115,\n",
       "  259,\n",
       "  962,\n",
       "  1305,\n",
       "  1243,\n",
       "  275,\n",
       "  508,\n",
       "  78,\n",
       "  618,\n",
       "  331,\n",
       "  1306,\n",
       "  1022,\n",
       "  1391,\n",
       "  759,\n",
       "  295],\n",
       " [1207, 78, 1189, 1313, 101, 325, 216],\n",
       " [1207, 78, 1189, 954, 1313, 325, 216],\n",
       " [203, 203, 203, 522, 247, 1391, 622, 1306, 78, 1189, 216],\n",
       " [1207, 867, 1391, 838, 815, 558, 194, 216],\n",
       " [867, 715, 1189, 585, 1150, 216],\n",
       " [1243, 1207, 867, 763, 1189, 431, 325],\n",
       " [1006, 618, 78, 1189, 115, 325],\n",
       " [763, 1006, 78, 800, 1189],\n",
       " [895, 622, 78, 39, 526, 551, 558, 118, 431, 956, 325, 203],\n",
       " [1207, 595, 763, 482, 66, 558, 203, 203, 203, 1401, 283, 1215, 216],\n",
       " [1062, 867, 445, 763, 1246, 277, 889, 377, 216],\n",
       " [539, 420, 1051, 1124, 865, 78, 1189, 115, 325],\n",
       " [1207, 867, 420, 1257, 558, 1215, 216],\n",
       " [763, 1124, 865, 429, 78, 293, 1150, 1391, 824, 889, 715, 1189, 203],\n",
       " [207, 522, 128, 1022, 715, 1189, 1407, 78, 1150, 618, 216],\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "questions_int=np.array(questions_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0 1187 ...   78  194  216]\n",
      " [   0    0    0 ...   72 1169  216]\n",
      " [   0    0    0 ... 1209  663  879]\n",
      " ...\n",
      " [1207   78  959 ...  800  431  611]\n",
      " [ 522 1041  522 ... 1168 1391  759]\n",
      " [ 207  522 1187 ...  193  203  203]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "\n",
    "features = pad_features(questions_int, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "#assert len(features)==len(sentrences.split('\\n')[:20]), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_set=list(set(answer))\n",
    "ans_to_int={}\n",
    "int_to_ans={}\n",
    "for i,j in enumerate(ans_set):\n",
    "    int_to_ans.update({i:j})\n",
    "for i,j in int_to_ans.items():\n",
    "    ans_to_int.update({j:i})\n",
    "master_intent=[]\n",
    "for i in answer:\n",
    "    master_intent.append(ans_to_int[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_intent[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(3263, 10) \n",
      "Validation set: \t(181, 10) \n",
      "Test set: \t\t(182, 10)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = master_intent[:split_idx], master_intent[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "valid_data = TensorDataset(torch.Tensor(val_x), torch.Tensor(val_y))\n",
    "test_data = TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 1\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([1, 10])\n",
      "Sample input: \n",
      " tensor([[   0.,    0.,    0.,    0.,    0.,    0.,    0.,   20., 1010.,  216.]])\n",
      "\n",
      "Sample label size:  torch.Size([1])\n",
      "Sample label: \n",
      " tensor([15.])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Semantic_Classifier(nn.Module):\n",
    "   \n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(Semantic_Classifier, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "       \n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        out = self.fc(torch.cat((torch.squeeze(hidden[0]),torch.squeeze(hidden[1])),0))\n",
    "       \n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic_Classifier(\n",
      "  (embedding): Embedding(1411, 300)\n",
      "  (lstm): LSTM(300, 256, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=19, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words_to_int)+1\n",
    "output_size = 19\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "# net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "net_1=Semantic_Classifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net_1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 1000... Loss: 2.031302214... Val Accuracy: 96.13%... Val Loss: 2.031017\n",
      "Epoch: 1/10... Step: 2000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.031022\n",
      "Epoch: 1/10... Step: 3000... Loss: 2.033260107... Val Accuracy: 94.48%... Val Loss: 2.031017\n",
      "Epoch: 2/10... Step: 4000... Loss: 2.031016588... Val Accuracy: 95.03%... Val Loss: 2.031021\n",
      "Epoch: 2/10... Step: 5000... Loss: 2.031016588... Val Accuracy: 95.58%... Val Loss: 2.031018\n",
      "Epoch: 2/10... Step: 6000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.044963\n",
      "Epoch: 3/10... Step: 7000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.055459\n",
      "Epoch: 3/10... Step: 8000... Loss: 2.031019688... Val Accuracy: 94.48%... Val Loss: 2.031017\n",
      "Epoch: 3/10... Step: 9000... Loss: 2.031021595... Val Accuracy: 95.03%... Val Loss: 2.031017\n",
      "Epoch: 4/10... Step: 10000... Loss: 2.110691547... Val Accuracy: 93.92%... Val Loss: 2.031029\n",
      "Epoch: 4/10... Step: 11000... Loss: 2.031021118... Val Accuracy: 95.03%... Val Loss: 2.110690\n",
      "Epoch: 4/10... Step: 12000... Loss: 2.031016588... Val Accuracy: 95.58%... Val Loss: 2.031017\n",
      "Epoch: 4/10... Step: 13000... Loss: 2.031017542... Val Accuracy: 95.58%... Val Loss: 2.031017\n",
      "Epoch: 5/10... Step: 14000... Loss: 2.031016588... Val Accuracy: 96.13%... Val Loss: 2.031017\n",
      "Epoch: 5/10... Step: 15000... Loss: 2.031017065... Val Accuracy: 95.03%... Val Loss: 2.031017\n",
      "Epoch: 5/10... Step: 16000... Loss: 2.031016588... Val Accuracy: 95.58%... Val Loss: 2.031017\n",
      "Epoch: 6/10... Step: 17000... Loss: 2.031016588... Val Accuracy: 95.58%... Val Loss: 2.031018\n",
      "Epoch: 6/10... Step: 18000... Loss: 2.031017542... Val Accuracy: 94.48%... Val Loss: 2.031018\n",
      "Epoch: 6/10... Step: 19000... Loss: 2.031017303... Val Accuracy: 95.03%... Val Loss: 2.031017\n",
      "Epoch: 7/10... Step: 20000... Loss: 2.031017780... Val Accuracy: 94.48%... Val Loss: 2.082215\n",
      "Epoch: 7/10... Step: 21000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.031107\n",
      "Epoch: 7/10... Step: 22000... Loss: 2.031016588... Val Accuracy: 96.13%... Val Loss: 2.031017\n",
      "Epoch: 8/10... Step: 23000... Loss: 2.031016588... Val Accuracy: 96.13%... Val Loss: 2.031017\n",
      "Epoch: 8/10... Step: 24000... Loss: 2.031016588... Val Accuracy: 96.13%... Val Loss: 2.031034\n",
      "Epoch: 8/10... Step: 25000... Loss: 2.031025887... Val Accuracy: 96.13%... Val Loss: 2.031093\n",
      "Epoch: 8/10... Step: 26000... Loss: 2.031016588... Val Accuracy: 95.03%... Val Loss: 2.032411\n",
      "Epoch: 9/10... Step: 27000... Loss: 2.031016588... Val Accuracy: 96.13%... Val Loss: 2.031022\n",
      "Epoch: 9/10... Step: 28000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.031167\n",
      "Epoch: 9/10... Step: 29000... Loss: 2.031016588... Val Accuracy: 95.58%... Val Loss: 2.031017\n",
      "Epoch: 10/10... Step: 30000... Loss: 2.031016588... Val Accuracy: 95.03%... Val Loss: 2.031017\n",
      "Epoch: 10/10... Step: 31000... Loss: 2.031016588... Val Accuracy: 94.48%... Val Loss: 2.031017\n",
      "Epoch: 10/10... Step: 32000... Loss: 2.031016588... Val Accuracy: 95.03%... Val Loss: 2.031017\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net_1.cuda()\n",
    "\n",
    "net_1.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    net_1.train()\n",
    "    h = net_1.init_hidden(batch_size)\n",
    "    \n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            inputs=torch.tensor(inputs).to(torch.int64)\n",
    "            labels=torch.tensor(labels).to(torch.long)\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net_1.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        net_1.train()\n",
    "        output, h = net_1(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net_1.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            net_1.eval()\n",
    "            val_h = net_1.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            a=[]\n",
    "            net_1.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    inputs=torch.tensor(inputs).to(torch.int64)\n",
    "                    labels=torch.tensor(labels).to(torch.long)\n",
    "\n",
    "                output, val_h = net_1(inputs, val_h)\n",
    "                _,index=torch.topk(output,1)\n",
    "                val_loss = criterion(output, labels)\n",
    "                a.append(index.item()==labels.item())\n",
    "            a=100*((a.count(1))/(a.count(0)+a.count(1)))\n",
    "                \n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.9f}...\".format(loss.item()),\n",
    "                  \"Val Accuracy: {:.2f}%...\".format(a),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you need tell him more funds <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "will you you always feed us <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "will that be we be the provided with food during OUTPUT: FAQ.food\n",
      "who other should i talk to to OUTPUT: FAQ.contact_info\n",
      "and when can do i speak at <PERIOD> <PERIOD> <PERIOD> OUTPUT: JOIN.speaker\n",
      "but i am just a terrible family foodie <EXCLAMATION_MARK> will OUTPUT: FAQ.food\n",
      "so if is there a place appropriate for speaker in OUTPUT: JOIN.speaker\n",
      "what though what can fields do all things you need OUTPUT: JOIN.speaker\n",
      "why are ya we gonna be provided such food OUTPUT: FAQ.food\n",
      "when do you have long enough for funding <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "such as would accommodation be on free charge or paid OUTPUT: FAQ.accom\n",
      "as will the food would be better provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "i would also like to deliver us a long oral OUTPUT: JOIN.speaker\n",
      "how to eventually do i apply to be this almost OUTPUT: JOIN.speaker\n",
      "how much can today we apply for sponsorship <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "what will be presently already provided to buy these new OUTPUT: SQ.event_speakers\n",
      "how to sponsor golf on the saturday event <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "also <PERIOD> <PERIOD> <PERIOD> are there there any large accommodations OUTPUT: FAQ.accom\n",
      "â« a â« how well can i join to join OUTPUT: JOIN.speaker\n",
      "her other phone OUTPUT: FAQ.contact_info\n",
      "or do you actually provide accommodation <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "i can stay so we contact for you for possible OUTPUT: JOIN.sponsor\n",
      "but when will will there be any vegan food <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "one iam willing members to formally contribute money for arranging OUTPUT: JOIN.sponsor\n",
      "so who to contact him for their further human doubts OUTPUT: FAQ.contact_info\n",
      "but wherein would food be provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "how to initiate some contact here the general organisation committee OUTPUT: FAQ.contact_info\n",
      "and what is not the criteria are required <PERIOD> <PERIOD> OUTPUT: JOIN.speaker\n",
      "what about just your lunch <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "but will food not be provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "why even should shall i register <QUESTION_MARK> OUTPUT: FAQ.why_reg\n",
      "had contact with info <QUESTION_MARK> OUTPUT: FAQ.contact_info\n",
      "can a sponsor even just join <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "like exactly what'd s so special about hosting <PERIOD> <PERIOD> OUTPUT: FAQ.why_reg\n",
      "i want to also new sponsor organizing this event OUTPUT: JOIN.sponsor\n",
      "and could you always provide the precise details of specific OUTPUT: FAQ.contact_info\n",
      "at what if a speaker really wants to possibly help OUTPUT: JOIN.speaker\n",
      "whom to contact even if even i should really want OUTPUT: JOIN.speaker\n",
      "will eventually there be any suitable accommodation options for meeting OUTPUT: FAQ.accom\n",
      "<PERIOD> <PERIOD> <PERIOD> who should if i ever contact OUTPUT: FAQ.contact_info\n",
      "can all our members company possibly be a part member OUTPUT: JOIN.sponsor\n",
      "do how we get food and during the whole cultural OUTPUT: FAQ.food\n",
      "if is there any requirements yet above for maintaining your OUTPUT: JOIN.speaker\n",
      "but why should i come back for us this in OUTPUT: FAQ.why_reg\n",
      "i am really willing anyone below to give us all OUTPUT: JOIN.speaker\n",
      "exactly what exactly was about refreshments <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "what help will i ever gain out by suddenly joining OUTPUT: FAQ.why_reg\n",
      "can i go and live there outside the downtown washington OUTPUT: FAQ.accom\n",
      "where is food usually included OUTPUT: FAQ.food\n",
      "and would pizza'o restaurant like s be provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "why should i always always fund fund of your event OUTPUT: JOIN.sponsor\n",
      "why should never i always register <QUESTION_MARK> OUTPUT: FAQ.why_reg\n",
      "no but whom should i contact <QUESTION_MARK> OUTPUT: FAQ.contact_info\n",
      "from whom further do they i need to be contact OUTPUT: FAQ.contact_info\n",
      "why then also should we each register ourselves for the OUTPUT: FAQ.why_reg\n",
      "how are are there arrangements for two visiting vegetarians <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "surely we both would simply like to sponsor your homecoming OUTPUT: JOIN.sponsor\n",
      "so will there be any less free food <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "would future food making arrangements be free of the interest OUTPUT: FAQ.food\n",
      "i have did some more general job a query then OUTPUT: FAQ.contact_info\n",
      "then can may i please serve come as a speaker OUTPUT: JOIN.speaker\n",
      "what exactly are you willing to please or speak some OUTPUT: JOIN.speaker\n",
      "also is there any vacancy out here looking for more OUTPUT: JOIN.speaker\n",
      "what are about the benefits for having me start attending OUTPUT: FAQ.why_reg\n",
      "those whom better advised to contact for more information OUTPUT: FAQ.contact_info\n",
      "for me who do think i contact him to regarding OUTPUT: FAQ.contact_info\n",
      "want us to further sponsor your event OUTPUT: JOIN.sponsor\n",
      "i more personally would like her offer to sponsor opencon OUTPUT: JOIN.sponsor\n",
      "how to be at a prominent and part outside of OUTPUT: JOIN.speaker\n",
      "or if is there non - korean vegetarian comfort food OUTPUT: FAQ.food\n",
      "where will there truly be stalls <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "what or what will and i get out of of OUTPUT: FAQ.why_reg\n",
      "why also should <PERIOD> <PERIOD> <PERIOD> i deliberately choose opencon OUTPUT: FAQ.why_reg\n",
      "what more about ending our stay OUTPUT: FAQ.accom\n",
      "and would you still behave like asked to discuss more OUTPUT: JOIN.sponsor\n",
      "whom exactly can be i contact <QUESTION_MARK> OUTPUT: FAQ.contact_info\n",
      "where from now can i stay <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "what will think i cannot really get out of and OUTPUT: FAQ.why_reg\n",
      "will have proper refreshments provide be provided OUTPUT: FAQ.food\n",
      "then will win this event be worth wallet <QUESTION_MARK> OUTPUT: FAQ.why_reg\n",
      "why should if not we register <QUESTION_MARK> OUTPUT: FAQ.why_reg\n",
      "i also want me to probably be a speaker host OUTPUT: JOIN.speaker\n",
      "what variety <PERIOD> <PERIOD> <PERIOD> are the foods actually offered OUTPUT: FAQ.food\n",
      "how true but can someone be a slightly better speaker OUTPUT: JOIN.speaker\n",
      "i would really like them to better sponsor some you OUTPUT: JOIN.sponsor\n",
      "for how is my it gone and a help depends OUTPUT: FAQ.why_reg\n",
      "i too would particularly like half them ready to speak OUTPUT: JOIN.speaker\n",
      "what is accommodation really free <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "what food arrangements <PERIOD> <PERIOD> <PERIOD> shall he be taken OUTPUT: FAQ.food\n",
      "and what benefits need that would enable i get to OUTPUT: FAQ.why_reg\n",
      "is your liquid finance done but just get back to OUTPUT: JOIN.sponsor\n",
      "whom i can suggest we eventually contact him again for OUTPUT: FAQ.contact_info\n",
      "why should i even register automatically for suddenly this big OUTPUT: FAQ.why_reg\n",
      "i never want to be sponsor OUTPUT: JOIN.sponsor\n",
      "is awaiting additional accommodation provided OUTPUT: FAQ.accom\n",
      "or why should am i register to file for no OUTPUT: FAQ.why_reg\n",
      "how often will they can and we not be properly OUTPUT: FAQ.accom\n",
      "wherever will accommodation now be arranged <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "that is even there any actual proper organic food arrangement OUTPUT: FAQ.food\n",
      "what is precisely the benefit ahead ahead of attending this OUTPUT: FAQ.why_reg\n",
      "so is there or there also any slot available locally OUTPUT: JOIN.speaker\n",
      "who can say i now ever call for in our OUTPUT: FAQ.contact_info\n",
      "will food often be warmly provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "but why should have i come alive only to cancel OUTPUT: FAQ.why_reg\n",
      "whom can now have i contact in life this special OUTPUT: FAQ.contact_info\n",
      "when will a food be provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "how soon do i ever <PERIOD> <PERIOD> <PERIOD> reach for OUTPUT: JOIN.sponsor\n",
      "but how more can i sponsor this famous event OUTPUT: JOIN.sponsor\n",
      "<PERIOD> <PERIOD> <PERIOD> what diseases are are you willing willing OUTPUT: JOIN.sponsor\n",
      "will food be present when provided OUTPUT: FAQ.food\n",
      "but well what are the current accommodations provided <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "there at all are many unique national events why should OUTPUT: FAQ.why_reg\n",
      "i i would also greatly like to now be onboard OUTPUT: JOIN.speaker\n",
      "what point is special about not reaching outside the event OUTPUT: FAQ.why_reg\n",
      "or <PERIOD> <PERIOD> <PERIOD> what are making the arrangements for OUTPUT: FAQ.food\n",
      "i could still just have some further doubts i want OUTPUT: FAQ.contact_info\n",
      "what time would at all it take be ac / OUTPUT: FAQ.accom\n",
      "what does this new place do well for me OUTPUT: FAQ.why_reg\n",
      "now how can i reach into anything the news event OUTPUT: FAQ.contact_info\n",
      "depending who they can tell do i contact or anyone OUTPUT: FAQ.contact_info\n",
      "after all it fit the details of the office accommodation OUTPUT: FAQ.accom\n",
      "will u provide available overnight accommodation or during the main OUTPUT: FAQ.accom\n",
      "how to sponsor the super football bowl event <QUESTION_MARK> OUTPUT: JOIN.sponsor\n",
      "do do we ever have to get past our because OUTPUT: FAQ.food\n",
      "like with what's so freaking incredibly special out tonight about OUTPUT: FAQ.why_reg\n",
      "then i simply would like you someone to willingly join OUTPUT: JOIN.sponsor\n",
      "any certain future benefits for funding activities of your own OUTPUT: JOIN.sponsor\n",
      "can i she i not stay often with a friend OUTPUT: FAQ.accom\n",
      "who cares ever can i really get contact for my OUTPUT: FAQ.contact_info\n",
      "or what book is also so incredibly special written in OUTPUT: FAQ.why_reg\n",
      "or is there enough food <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "what size is the current accommodation requirement situation <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "now where will insist i stay <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "what better form of outside help are your all a OUTPUT: JOIN.sponsor\n",
      "can still we i get some numbers OUTPUT: FAQ.contact_info\n",
      "oh do ya do know we already get the right OUTPUT: SQ.event_prize\n",
      "what came about certain free food / video travel refreshments OUTPUT: FAQ.food\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how then can i help someone with the event OUTPUT: JOIN.sponsor\n",
      "tell them whom to contact <QUESTION_MARK> OUTPUT: FAQ.contact_info\n",
      "to where can should i stay <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "of what number is the menu <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "because how many do know we now have to pay OUTPUT: SQ.reg_fee\n",
      "what point is the procedure to finally actually become a OUTPUT: JOIN.speaker\n",
      "say what are th end arrangements u for food OUTPUT: FAQ.food\n",
      "so i want to <PERIOD> <PERIOD> <PERIOD> speak OUTPUT: JOIN.speaker\n",
      "but what else do we now achieve just by the OUTPUT: FAQ.why_reg\n",
      "had i interested in sponsoring him quite the event OUTPUT: JOIN.sponsor\n",
      "give me set up the contact info of meeting someone OUTPUT: FAQ.contact_info\n",
      "how many can only i speak please at your event OUTPUT: JOIN.speaker\n",
      "then why should i dare come <QUESTION_MARK> OUTPUT: FAQ.why_reg\n",
      "who should would i possibly contact for accommodation from health OUTPUT: FAQ.contact_info\n",
      "for how much can someone truly be a bad sponsor OUTPUT: JOIN.sponsor\n",
      "who can a speaker not join <QUESTION_MARK> OUTPUT: JOIN.speaker\n",
      "where are they there there arrangements made for non african OUTPUT: FAQ.food\n",
      "and what changes there if a sponsor really wants to OUTPUT: JOIN.sponsor\n",
      "i need more for a help OUTPUT: FAQ.contact_info\n",
      "who who can shall i now contact for immediate queries OUTPUT: FAQ.contact_info\n",
      "well what good r we never gonna get from al OUTPUT: FAQ.why_reg\n",
      "why was this event and what not before why followed OUTPUT: FAQ.why_reg\n",
      "whom was to ever contact me for sponsorship of purposes OUTPUT: JOIN.sponsor\n",
      "plus who and then should i maybe call contact for OUTPUT: FAQ.contact_info\n",
      "how odd to be so a part accused of securing OUTPUT: JOIN.speaker\n",
      "will this someday we provided two rooms between downstairs or OUTPUT: FAQ.accom\n",
      "where do whether i eventually stay <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "what just is most interesting <PERIOD> <PERIOD> <PERIOD> about this OUTPUT: FAQ.why_reg\n",
      "will their food be ever provided <QUESTION_MARK> OUTPUT: FAQ.food\n",
      "can i actually actually become a speaker OUTPUT: JOIN.speaker\n",
      "but i <PERIOD> <PERIOD> <PERIOD> just would still like to OUTPUT: JOIN.speaker\n",
      "and contact details number of organisers OUTPUT: FAQ.contact_info\n",
      "why should i do i only secretly register for this OUTPUT: FAQ.why_reg\n",
      "whom anybody can say i contact just now for more OUTPUT: FAQ.contact_info\n",
      "or how can both you advertise her for hurting me OUTPUT: JOIN.sponsor\n",
      "where can say to i register all ourselves as sponsors OUTPUT: JOIN.sponsor\n",
      "but maybe i still want even more personal personal details OUTPUT: FAQ.contact_info\n",
      "why should then we not register this everyone for hosting OUTPUT: FAQ.why_reg\n",
      "can <PERIOD> <PERIOD> <PERIOD> would i stay long outside campus OUTPUT: FAQ.accom\n",
      "surely would accommodation indeed be something often best provided in OUTPUT: FAQ.accom\n",
      "just what are the benefits ahead in sponsoring a host OUTPUT: JOIN.sponsor\n",
      "why should i register opposite her for just this upcoming OUTPUT: FAQ.why_reg\n",
      "who can argue i but contact you for more comprehensive OUTPUT: FAQ.contact_info\n",
      "and would private transportation to college and campus be provided OUTPUT: JOIN.speaker\n",
      "and is it not there any general email for id OUTPUT: FAQ.contact_info\n",
      "Val Loss: 2.061944         Val_Acuuracy: 98.35164835164835\n"
     ]
    }
   ],
   "source": [
    "net_1.eval()\n",
    "val_h = net_1.init_hidden(batch_size)\n",
    "val_losses = []\n",
    "a=[]\n",
    "net_1.eval()\n",
    "for inputs, labels in test_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                inputs=torch.tensor(inputs).to(torch.int64)\n",
    "                labels=torch.tensor(labels).to(torch.long)\n",
    "                \n",
    "        output, val_h = net_1(inputs, val_h)\n",
    "        _,index=torch.topk(output,1)\n",
    "        a.append(index.item()==labels.item())\n",
    "        val_loss = criterion(output, labels)\n",
    "        for i in inputs.squeeze():\n",
    "            if i:\n",
    "                print(int_to_words[i.item()],end=\" \")\n",
    "        print('OUTPUT:',int_to_ans[index.item()])\n",
    "                \n",
    "        \n",
    "        \n",
    "        val_losses.append(val_loss.item())\n",
    "print( \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\"        Val_Acuuracy:\",(a.count(1)/(a.count(0)+a.count(1)))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Guru Aathavan AL U\\code\\Ana_conda\\envs\\pytorch_cuda_trail\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perhaps but what is the reward <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "perhaps what will ever i get <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "one when does the loop even start of and and OUTPUT: SQ.event_date\n",
      "besides when will ever the speakers not actually be coming OUTPUT: SQ.event_speakers\n",
      "<PERIOD> <PERIOD> <PERIOD> what's the proper event time and have OUTPUT: SQ.event_date\n",
      "now when when is it <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "and will the only prize awarded be from cash or OUTPUT: SQ.event_prize\n",
      "what is the maximum planned time release this span allowed OUTPUT: SQ.event_date\n",
      "what or other domains / other specializations else are all OUTPUT: SQ.event_speakers\n",
      "then what whatas there were in the time table for OUTPUT: SQ.event_schedule\n",
      "who are the new main speakers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "but do the normal people running away who maybe didn't OUTPUT: SQ.event_prize\n",
      "no internal speakers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "what is against the current last registration date set up OUTPUT: SQ.reg_lastdate\n",
      "is there a first person winner and prize <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "but what is this all about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "what often follows is the timeline of analyzing for the OUTPUT: SQ.event_schedule\n",
      "when here is the event event already planned or to OUTPUT: SQ.event_date\n",
      "basically what is is this it just during a conference OUTPUT: SQ.event_schedule\n",
      "so what will we do for them water accommodation OUTPUT: FAQ.accom\n",
      "that or else are there there any prizes for winners OUTPUT: SQ.event_prize\n",
      "do runner ups get punished for saying something <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "and on which dates or will the current event be OUTPUT: SQ.event_date\n",
      "what corporation is currently controlling the fall schedule for the OUTPUT: SQ.event_date\n",
      "what are the bigger prize prizes like for movie winners OUTPUT: SQ.event_prize\n",
      "what is it for the prize and money <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "and which current day to which a future day is OUTPUT: SQ.event_date\n",
      "s what'd s the schedule OUTPUT: SQ.event_schedule\n",
      "when exactly is it officially the event scheduled <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "what importance where is the usual precise flow of events OUTPUT: SQ.event_schedule\n",
      "but when is the international conference <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "their usual event itinerary <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "will first we get the stickers for please answering commonly OUTPUT: SQ.event_prize\n",
      "4 or how very many speakers <PERIOD> <PERIOD> <PERIOD> will OUTPUT: SQ.event_speakers\n",
      "what's next to the outstanding prize box money <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "where can i just see on here the event schedule OUTPUT: SQ.event_schedule\n",
      "what but when is this event <QUESTION_MARK> to do everything OUTPUT: SQ.event_date\n",
      "whatas the new construction schedule <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "when i would that i be provided additional building accommodation OUTPUT: FAQ.accom\n",
      "when nowhere else are the normal events happening <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "how infinitely instead many possible days of the event would OUTPUT: SQ.event_schedule\n",
      "explain what will decide the winner get OUTPUT: SQ.event_prize\n",
      "since when who is the event <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "<PERIOD> <PERIOD> <PERIOD> what exactly is of this upcoming event OUTPUT: SQ.event_details\n",
      "what precisely is now the deadline for web account registration OUTPUT: SQ.reg_lastdate\n",
      "in what are also the dates typical of performing the OUTPUT: SQ.event_date\n",
      "what's the real great society event <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "who are the keynote speakers today included today in this OUTPUT: SQ.event_speakers\n",
      "and when it is the event organised OUTPUT: SQ.event_date\n",
      "and what work is with the cash prize <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "since but when will that event will really take place OUTPUT: SQ.event_date\n",
      "whom could award <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "so which fields are supposedly being fully covered <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "when just when is this the appropriate event <QUESTION_MARK> can OUTPUT: SQ.event_schedule\n",
      "exactly what information is this this event about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "for now who truly will be the speaker <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "and when and here is the first event being hosted OUTPUT: SQ.event_schedule\n",
      "what is the event there we all constantly fuss about OUTPUT: SQ.event_details\n",
      "what does exist is the detailed event release schedule OUTPUT: SQ.event_schedule\n",
      "what bad news is this OUTPUT: SQ.event_details\n",
      "<PERIOD> <PERIOD> <PERIOD> then what is with the first itinerary OUTPUT: SQ.event_schedule\n",
      "holes in what is this <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "thus is internal accomadation available simply as for weak externals OUTPUT: FAQ.accom\n",
      "what is the tomorrow dance schedule <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "what time <PERIOD> <PERIOD> <PERIOD> is this about OUTPUT: SQ.event_details\n",
      "then will the accommodation costs costs be included again in OUTPUT: FAQ.accom\n",
      "and but what <PERIOD> <PERIOD> <PERIOD> about accomodation for taking OUTPUT: FAQ.accom\n",
      "how how quickly will you accommodate <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "and so how longer would the main event go on OUTPUT: SQ.event_schedule\n",
      "guess who exactly there is ever going to address the OUTPUT: SQ.event_speakers\n",
      "what bad do can i truly get if only i OUTPUT: SQ.event_prize\n",
      "complete concert schedule OUTPUT: SQ.event_schedule\n",
      "both who are also the speakers OUTPUT: SQ.event_speakers\n",
      "so when is when the event OUTPUT: SQ.event_date\n",
      "guess who we will be speaking at ten for the OUTPUT: SQ.event_speakers\n",
      "all who <PERIOD> <PERIOD> <PERIOD> are the main speakers <EXCLAMATION_MARK> OUTPUT: SQ.event_speakers\n",
      "is accommodation well enough provided <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "so what yet is the schedule OUTPUT: SQ.event_schedule\n",
      "who all will be coming right up to give first OUTPUT: SQ.event_speakers\n",
      "just when is such it going to really be getting OUTPUT: SQ.event_date\n",
      "who all are of the speakers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "what is this famous event truly about OUTPUT: SQ.event_details\n",
      "what about all the female trainers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "will <PERIOD> <PERIOD> <PERIOD> i get some more ridiculous bounty OUTPUT: SQ.event_prize\n",
      "when is it all gonna never start OUTPUT: SQ.event_schedule\n",
      "or the whatas the last deadline then for my college OUTPUT: SQ.reg_lastdate\n",
      "who really is coming to visit the student conference OUTPUT: SQ.event_speakers\n",
      "simply what are the expected rewards <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "what is making this awful whole event about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "so when to is it scheduled <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "see who someone is speaking OUTPUT: SQ.event_speakers\n",
      "so who was all farther along exactly are the speakers OUTPUT: SQ.event_speakers\n",
      "what makes right is this all all about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "but not until fourth when can i register <QUESTION_MARK> OUTPUT: SQ.reg_lastdate\n",
      "<PERIOD> <PERIOD> <PERIOD> how will getting the prize eventually be OUTPUT: SQ.event_prize\n",
      "so do we still have orders to also stay on OUTPUT: SQ.event_schedule\n",
      "so on what dates are for the events going today OUTPUT: SQ.event_date\n",
      "why because what are of the dates he set for OUTPUT: SQ.event_date\n",
      "how is everybody holding on it really different being just OUTPUT: SQ.event_date\n",
      "then what news can i get <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "prizes for the challenge book winners <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "but what is the tour your itinerary OUTPUT: SQ.event_schedule\n",
      "but how very long many days the actual event would OUTPUT: SQ.event_date\n",
      "precisely what city colour is the event about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "what'the lot s else is just the prize <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "where can am sure i stay OUTPUT: FAQ.accom\n",
      "can i <PERIOD> <PERIOD> <PERIOD> please see a list of OUTPUT: SQ.event_speakers\n",
      "who are the most current guests OUTPUT: SQ.event_speakers\n",
      "are we short there of second on points and third OUTPUT: SQ.event_prize\n",
      "do did we get lucky any more prizes <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "where can help myself i stay <QUESTION_MARK> OUTPUT: FAQ.accom\n",
      "but what is at the last appropriate date fixed for OUTPUT: SQ.reg_lastdate\n",
      "but when is last it being being held <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "they decide then what is the main prize for the OUTPUT: SQ.event_prize\n",
      "what good is the criteria employed for also getting into OUTPUT: SQ.reg_fee\n",
      "when time exists is the event happening OUTPUT: SQ.event_date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when is even even the same event going by to OUTPUT: SQ.event_date\n",
      "yet what is open the con <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "where will help we still get t - face shirts OUTPUT: SQ.event_prize\n",
      "do <PERIOD> <PERIOD> <PERIOD> you actually have any outstanding prizes OUTPUT: SQ.event_prize\n",
      "when of now is it OUTPUT: SQ.event_date\n",
      "only later can we then approach anyone for registration any OUTPUT: SQ.reg_lastdate\n",
      "it just what does it entail OUTPUT: SQ.event_details\n",
      "when of and where it is all was of it OUTPUT: SQ.event_date\n",
      "on what precisely those specified dates will the public sporting OUTPUT: SQ.event_date\n",
      "what is en vogue opencon <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "what good is timing of already winning the event OUTPUT: SQ.event_schedule\n",
      "what natural one would that be the proper last service OUTPUT: SQ.reg_lastdate\n",
      "could you please name the latest speakers from who now OUTPUT: SQ.event_speakers\n",
      "then till what proposed date can i apply you for OUTPUT: SQ.reg_lastdate\n",
      "now what is from all the event about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "what better date <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "when when is the last long day already starting to OUTPUT: SQ.reg_lastdate\n",
      "but till precisely which date can i ever register OUTPUT: SQ.reg_lastdate\n",
      "when is coming this extraordinary event <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "now what will i ever get if i ever can OUTPUT: SQ.event_prize\n",
      "if are we there cash prizes <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "can you remember i have read the whole usual itinerary OUTPUT: SQ.event_schedule\n",
      "will not this hidden speaker be back there <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "what reasons are there for the prizes being also offered OUTPUT: SQ.event_prize\n",
      "how is she putting the event to schedule <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "who will be taking the opening prayer after sessions OUTPUT: SQ.event_speakers\n",
      "but can we register on their designated spot <QUESTION_MARK> OUTPUT: SQ.reg_lastdate\n",
      "what exactly will we happen when <QUESTION_MARK> OUTPUT: SQ.event_schedule\n",
      "but what is the consolation prize and money <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "<PERIOD> <PERIOD> <PERIOD> what year is thus the competition schedule OUTPUT: SQ.event_schedule\n",
      "where and what exactly is open con OUTPUT: SQ.event_details\n",
      "do you kids have any speakers worth ever coming directly OUTPUT: SQ.event_speakers\n",
      "on other what different days now we have the actual OUTPUT: SQ.event_date\n",
      "so whatas so fucking great involved in hosting this particular OUTPUT: SQ.event_details\n",
      "when or is after this the last date <QUESTION_MARK> OUTPUT: SQ.reg_lastdate\n",
      "before then who would be presiding as only speakers wait OUTPUT: SQ.event_speakers\n",
      "what is just the time flow of processing the new OUTPUT: SQ.event_schedule\n",
      "will there always not be real goodies <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "what is just this all about <QUESTION_MARK> OUTPUT: SQ.event_details\n",
      "is stay longer provided always for us OUTPUT: FAQ.accom\n",
      "are there <PERIOD> <PERIOD> <PERIOD> any any potential prizes <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "and who all are of among the potential prospective speakers OUTPUT: SQ.event_speakers\n",
      "from what are only exactly those the dates for when OUTPUT: SQ.event_date\n",
      "what will ever mean we actually do in fact planning OUTPUT: SQ.event_details\n",
      "now who are now the speakers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "that general with info about seeing the speakers <QUESTION_MARK> OUTPUT: SQ.event_speakers\n",
      "does it beating the winner necessarily get a cash prize OUTPUT: SQ.event_prize\n",
      "in at least how frequently is the accomodation really done OUTPUT: FAQ.accom\n",
      "see just what is on this the last day scheduled OUTPUT: SQ.reg_lastdate\n",
      "last recorded date to attendance register OUTPUT: SQ.reg_lastdate\n",
      "what great benefits do we get hidden away than from OUTPUT: SQ.event_details\n",
      "when do we you first see need any my presence OUTPUT: SQ.event_date\n",
      "or what character is the breakdown method for winning all OUTPUT: SQ.event_prize\n",
      "only what there else really is the event all about OUTPUT: SQ.event_details\n",
      "just when else will opencon 1 be more held <QUESTION_MARK> OUTPUT: SQ.event_date\n",
      "who are the speakers for judging the first current event OUTPUT: SQ.event_speakers\n",
      "when exactly is tomorrow the last job date gone to OUTPUT: SQ.reg_lastdate\n",
      "and what needs is the itinerary track of your future OUTPUT: SQ.event_schedule\n",
      "now what exactly else is opencon basically good talking about OUTPUT: SQ.event_details\n",
      "what good cause is this event about OUTPUT: SQ.event_details\n",
      "oh so how much is now the prize money OUTPUT: SQ.event_prize\n",
      "who are named for the selected featured inspirational speakers for OUTPUT: SQ.event_speakers\n",
      "can we talk again over to his <PERIOD> <PERIOD> <PERIOD> OUTPUT: SQ.event_speakers\n",
      "but so now when is the list last for registration OUTPUT: SQ.reg_lastdate\n",
      "what about us paying back the price money <QUESTION_MARK> OUTPUT: SQ.event_prize\n",
      "Val Loss: 2.070140         Val_Acuuracy: 95.58011049723757\n"
     ]
    }
   ],
   "source": [
    "net_1.eval()\n",
    "val_h = net_1.init_hidden(batch_size)\n",
    "val_losses = []\n",
    "a=[]\n",
    "net_1.eval()\n",
    "for inputs, labels in valid_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                inputs=torch.tensor(inputs).to(torch.int64)\n",
    "                labels=torch.tensor(labels).to(torch.long)\n",
    "                \n",
    "        output, val_h = net_1(inputs, val_h)\n",
    "        _,index=torch.topk(output,1)\n",
    "        a.append(index.item()==labels.item())\n",
    "        val_loss = criterion(output, labels)\n",
    "        for i in inputs.squeeze():\n",
    "            if i:\n",
    "                print(int_to_words[i.item()],end=\" \")\n",
    "        print('OUTPUT:',int_to_ans[index.item()])\n",
    "                \n",
    "        \n",
    "        \n",
    "        val_losses.append(val_loss.item())\n",
    "print( \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\"        Val_Acuuracy:\",(a.count(1)/(a.count(0)+a.count(1)))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Semantic_Classifier(\n",
       "  (embedding): Embedding(1411, 300)\n",
       "  (lstm): LSTM(300, 256, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=19, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_1.state_dict(),\"net_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
