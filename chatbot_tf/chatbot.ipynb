{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "    print(df.head())\n",
    "    intent = df[\"Intent\"]\n",
    "    unique_intent = list(set(intent))\n",
    "    sentences = list(df[\"Sentence\"])\n",
    "    return (intent, unique_intent, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Sentence  Intent\n",
      "0  Is there a bot chatting to me?  GQ.bot\n",
      "1        Is it automated message?  GQ.bot\n",
      "2             Computer based pely  GQ.bot\n",
      "3                   Bot or human?  GQ.bot\n",
      "4        Bot is chatting with me?  GQ.bot\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, sentences = load_dataset(\"cb_dataset_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's the club about?\", 'What does IEEE-VIT does', 'What is IEEE-VIT?', 'What is ieee', 'What is IEEE vit ?']\n",
      "['FAQ.why_reg', 'SQ.IEEE', 'SQ.reg_fee', 'GQ.name', 'FAQ.food', 'GQ.query', 'GQ.bot', 'SQ.event_details', 'GQ.gen', 'GQ.help', 'SQ.event_prize', 'FAQ.contact_info', 'JOIN.speaker', 'JOIN.sponsor', 'SQ.event_speakers', 'SQ.event_date', 'SQ.reg_lastdate', 'SQ.event_schedule', 'FAQ.accom']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[100:105])\n",
    "print(unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()  #using lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'registration', 'fee', 'of', 'the', 'event'], ['what', 'is', 'the', 'fees', 'required', 'to', 'register', 'for', 'the', 'event'], ['what', 's', 'the', 'price', 'for', 'getting', 'a', 'registration', 'done', 'in', 'the', 'event']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_words[115:118])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    return(len(max(words, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 418 and Maximum length = 16\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   3,   1,  29,  93,  23,   1,   5,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  2,   3,   1,  78, 163,   7,  22,   6,   1,   5,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  2,  34,   1, 164,   6, 165,  11,  29, 131,  30,   1,   5,   0,\n",
       "          0,   0,   0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc[115:118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (518, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faq.why_reg': 1,\n",
       " 'sq.ieee': 2,\n",
       " 'sq.reg_fee': 3,\n",
       " 'gq.name': 4,\n",
       " 'faq.food': 5,\n",
       " 'gq.query': 6,\n",
       " 'gq.bot': 7,\n",
       " 'sq.event_details': 8,\n",
       " 'gq.gen': 9,\n",
       " 'gq.help': 10,\n",
       " 'sq.event_prize': 11,\n",
       " 'faq.contact_info': 12,\n",
       " 'join.speaker': 13,\n",
       " 'join.sponsor': 14,\n",
       " 'sq.event_speakers': 15,\n",
       " 'sq.event_date': 16,\n",
       " 'sq.reg_lastdate': 17,\n",
       " 'sq.event_schedule': 18,\n",
       " 'faq.accom': 19}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse = False)\n",
    "    return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
    "#x_train, x_val, y_train, y_val = train_test_split(padded_doc, output_one_hot, test_size = 0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (414, 16) and train_Y = (414, 19)\n",
      "Shape of val_X = (104, 16) and val_Y = (104, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (x_train.shape, y_train.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(19, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 310 samples, validate on 104 samples\n",
      "Epoch 1/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 2.9361 - accuracy: 0.0664 \n",
      "Epoch 00001: val_loss improved from inf to 2.95640, saving model to model.h5\n",
      "310/310 [==============================] - 5s 17ms/sample - loss: 2.9411 - accuracy: 0.0581 - val_loss: 2.9564 - val_accuracy: 0.0481\n",
      "Epoch 2/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 2.9315 - accuracy: 0.0664\n",
      "Epoch 00002: val_loss improved from 2.95640 to 2.94393, saving model to model.h5\n",
      "310/310 [==============================] - 0s 407us/sample - loss: 2.9289 - accuracy: 0.0677 - val_loss: 2.9439 - val_accuracy: 0.0481\n",
      "Epoch 3/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 2.9200 - accuracy: 0.0820\n",
      "Epoch 00003: val_loss improved from 2.94393 to 2.93746, saving model to model.h5\n",
      "310/310 [==============================] - 0s 395us/sample - loss: 2.9151 - accuracy: 0.0839 - val_loss: 2.9375 - val_accuracy: 0.0481\n",
      "Epoch 4/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 2.9004 - accuracy: 0.1007\n",
      "Epoch 00004: val_loss improved from 2.93746 to 2.92004, saving model to model.h5\n",
      "310/310 [==============================] - 0s 407us/sample - loss: 2.9067 - accuracy: 0.0935 - val_loss: 2.9200 - val_accuracy: 0.1250\n",
      "Epoch 5/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 2.8578 - accuracy: 0.1562\n",
      "Epoch 00005: val_loss improved from 2.92004 to 2.89271, saving model to model.h5\n",
      "310/310 [==============================] - 0s 386us/sample - loss: 2.8613 - accuracy: 0.1516 - val_loss: 2.8927 - val_accuracy: 0.0577\n",
      "Epoch 6/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 2.7778 - accuracy: 0.1528\n",
      "Epoch 00006: val_loss improved from 2.89271 to 2.84921, saving model to model.h5\n",
      "310/310 [==============================] - 0s 384us/sample - loss: 2.7737 - accuracy: 0.1581 - val_loss: 2.8492 - val_accuracy: 0.0769\n",
      "Epoch 7/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 2.6657 - accuracy: 0.2049\n",
      "Epoch 00007: val_loss improved from 2.84921 to 2.70517, saving model to model.h5\n",
      "310/310 [==============================] - 0s 403us/sample - loss: 2.6548 - accuracy: 0.2161 - val_loss: 2.7052 - val_accuracy: 0.1250\n",
      "Epoch 8/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 2.4492 - accuracy: 0.1914\n",
      "Epoch 00008: val_loss improved from 2.70517 to 2.45589, saving model to model.h5\n",
      "310/310 [==============================] - 0s 402us/sample - loss: 2.4608 - accuracy: 0.1935 - val_loss: 2.4559 - val_accuracy: 0.2885\n",
      "Epoch 9/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 2.2974 - accuracy: 0.2535\n",
      "Epoch 00009: val_loss improved from 2.45589 to 2.32765, saving model to model.h5\n",
      "310/310 [==============================] - 0s 416us/sample - loss: 2.3083 - accuracy: 0.2484 - val_loss: 2.3276 - val_accuracy: 0.3173\n",
      "Epoch 10/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 2.0196 - accuracy: 0.3164\n",
      "Epoch 00010: val_loss improved from 2.32765 to 2.06433, saving model to model.h5\n",
      "310/310 [==============================] - 0s 416us/sample - loss: 1.9776 - accuracy: 0.3419 - val_loss: 2.0643 - val_accuracy: 0.4712\n",
      "Epoch 11/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 1.7846 - accuracy: 0.4097\n",
      "Epoch 00011: val_loss improved from 2.06433 to 1.79244, saving model to model.h5\n",
      "310/310 [==============================] - 0s 406us/sample - loss: 1.7801 - accuracy: 0.4097 - val_loss: 1.7924 - val_accuracy: 0.5000\n",
      "Epoch 12/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 1.6067 - accuracy: 0.4444\n",
      "Epoch 00012: val_loss did not improve from 1.79244\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 1.6169 - accuracy: 0.4419 - val_loss: 1.8500 - val_accuracy: 0.4519\n",
      "Epoch 13/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 1.5422 - accuracy: 0.4861\n",
      "Epoch 00013: val_loss improved from 1.79244 to 1.55002, saving model to model.h5\n",
      "310/310 [==============================] - 0s 400us/sample - loss: 1.5259 - accuracy: 0.4935 - val_loss: 1.5500 - val_accuracy: 0.5577\n",
      "Epoch 14/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 1.4274 - accuracy: 0.5312\n",
      "Epoch 00014: val_loss did not improve from 1.55002\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 1.4488 - accuracy: 0.5032 - val_loss: 1.6537 - val_accuracy: 0.5385\n",
      "Epoch 15/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 1.2346 - accuracy: 0.5781\n",
      "Epoch 00015: val_loss improved from 1.55002 to 1.54440, saving model to model.h5\n",
      "310/310 [==============================] - 0s 435us/sample - loss: 1.2237 - accuracy: 0.5645 - val_loss: 1.5444 - val_accuracy: 0.5288\n",
      "Epoch 16/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 1.1517 - accuracy: 0.5972\n",
      "Epoch 00016: val_loss improved from 1.54440 to 1.52008, saving model to model.h5\n",
      "310/310 [==============================] - 0s 394us/sample - loss: 1.1526 - accuracy: 0.5968 - val_loss: 1.5201 - val_accuracy: 0.5481\n",
      "Epoch 17/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 1.0084 - accuracy: 0.6840\n",
      "Epoch 00017: val_loss improved from 1.52008 to 1.21957, saving model to model.h5\n",
      "310/310 [==============================] - 0s 394us/sample - loss: 0.9871 - accuracy: 0.6935 - val_loss: 1.2196 - val_accuracy: 0.6923\n",
      "Epoch 18/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.8509 - accuracy: 0.7083\n",
      "Epoch 00018: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.8355 - accuracy: 0.7129 - val_loss: 1.5266 - val_accuracy: 0.5769\n",
      "Epoch 19/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.8201 - accuracy: 0.7118\n",
      "Epoch 00019: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 0.8223 - accuracy: 0.7161 - val_loss: 1.3380 - val_accuracy: 0.6154\n",
      "Epoch 20/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.7264 - accuracy: 0.7812\n",
      "Epoch 00020: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.7217 - accuracy: 0.7774 - val_loss: 1.2204 - val_accuracy: 0.6538\n",
      "Epoch 21/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.7005 - accuracy: 0.7743\n",
      "Epoch 00021: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.6931 - accuracy: 0.7806 - val_loss: 1.3553 - val_accuracy: 0.6250\n",
      "Epoch 22/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.6376 - accuracy: 0.7734\n",
      "Epoch 00022: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.5991 - accuracy: 0.7903 - val_loss: 1.2579 - val_accuracy: 0.6250\n",
      "Epoch 23/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.6465 - accuracy: 0.7986\n",
      "Epoch 00023: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.6539 - accuracy: 0.7968 - val_loss: 1.2509 - val_accuracy: 0.6346\n",
      "Epoch 24/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.4875 - accuracy: 0.8576\n",
      "Epoch 00024: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.4820 - accuracy: 0.8581 - val_loss: 1.2856 - val_accuracy: 0.6538\n",
      "Epoch 25/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.5834 - accuracy: 0.8090\n",
      "Epoch 00025: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.5639 - accuracy: 0.8161 - val_loss: 1.3386 - val_accuracy: 0.6635\n",
      "Epoch 26/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.5158 - accuracy: 0.8229\n",
      "Epoch 00026: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.5186 - accuracy: 0.8258 - val_loss: 1.3294 - val_accuracy: 0.6635\n",
      "Epoch 27/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.5015 - accuracy: 0.8090\n",
      "Epoch 00027: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.5149 - accuracy: 0.8065 - val_loss: 1.2697 - val_accuracy: 0.7019\n",
      "Epoch 28/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.5079 - accuracy: 0.8229\n",
      "Epoch 00028: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.5151 - accuracy: 0.8194 - val_loss: 1.4083 - val_accuracy: 0.6635\n",
      "Epoch 29/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.5155 - accuracy: 0.7500\n",
      "Epoch 00029: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 255us/sample - loss: 0.4016 - accuracy: 0.8581 - val_loss: 1.4650 - val_accuracy: 0.7115\n",
      "Epoch 30/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.4297 - accuracy: 0.8542\n",
      "Epoch 00030: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.4268 - accuracy: 0.8548 - val_loss: 1.6390 - val_accuracy: 0.6442\n",
      "Epoch 31/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.5080 - accuracy: 0.8264\n",
      "Epoch 00031: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.4956 - accuracy: 0.8258 - val_loss: 1.3261 - val_accuracy: 0.7019\n",
      "Epoch 32/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.4120 - accuracy: 0.8516\n",
      "Epoch 00032: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.3914 - accuracy: 0.8581 - val_loss: 1.4717 - val_accuracy: 0.6731\n",
      "Epoch 33/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.3664 - accuracy: 0.8867\n",
      "Epoch 00033: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.3536 - accuracy: 0.8968 - val_loss: 1.6005 - val_accuracy: 0.6827\n",
      "Epoch 34/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.3750 - accuracy: 0.8715\n",
      "Epoch 00034: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.3716 - accuracy: 0.8677 - val_loss: 1.4601 - val_accuracy: 0.6731\n",
      "Epoch 35/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.3488 - accuracy: 0.8715\n",
      "Epoch 00035: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.3573 - accuracy: 0.8645 - val_loss: 1.3378 - val_accuracy: 0.7500\n",
      "Epoch 36/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.4005 - accuracy: 0.8750\n",
      "Epoch 00036: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.4008 - accuracy: 0.8710 - val_loss: 1.5896 - val_accuracy: 0.7019\n",
      "Epoch 37/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.3503 - accuracy: 0.9375\n",
      "Epoch 00037: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 252us/sample - loss: 0.3476 - accuracy: 0.8839 - val_loss: 1.5588 - val_accuracy: 0.7115\n",
      "Epoch 38/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.3313 - accuracy: 0.8924\n",
      "Epoch 00038: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.3216 - accuracy: 0.8968 - val_loss: 1.5874 - val_accuracy: 0.6827\n",
      "Epoch 39/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.3372 - accuracy: 0.8819\n",
      "Epoch 00039: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.3207 - accuracy: 0.8903 - val_loss: 1.6941 - val_accuracy: 0.7019\n",
      "Epoch 40/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.3293 - accuracy: 0.8889\n",
      "Epoch 00040: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.3403 - accuracy: 0.8839 - val_loss: 1.6114 - val_accuracy: 0.6923\n",
      "Epoch 41/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.3059 - accuracy: 0.8906\n",
      "Epoch 00041: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 0.2865 - accuracy: 0.8968 - val_loss: 1.5591 - val_accuracy: 0.7019\n",
      "Epoch 42/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2242 - accuracy: 0.9236\n",
      "Epoch 00042: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.2260 - accuracy: 0.9226 - val_loss: 1.5953 - val_accuracy: 0.7019\n",
      "Epoch 43/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.2188 - accuracy: 0.9180\n",
      "Epoch 00043: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 0.2208 - accuracy: 0.9161 - val_loss: 1.6031 - val_accuracy: 0.7212\n",
      "Epoch 44/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.3267 - accuracy: 0.8750\n",
      "Epoch 00044: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.2759 - accuracy: 0.9000 - val_loss: 1.6718 - val_accuracy: 0.7115\n",
      "Epoch 45/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2780 - accuracy: 0.8993\n",
      "Epoch 00045: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 0.2845 - accuracy: 0.8935 - val_loss: 1.5828 - val_accuracy: 0.7115\n",
      "Epoch 46/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.2597 - accuracy: 0.9023\n",
      "Epoch 00046: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.2540 - accuracy: 0.9000 - val_loss: 1.6687 - val_accuracy: 0.7019\n",
      "Epoch 47/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.2990 - accuracy: 0.8906\n",
      "Epoch 00047: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.2971 - accuracy: 0.8839 - val_loss: 1.8921 - val_accuracy: 0.6923\n",
      "Epoch 48/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2395 - accuracy: 0.9167\n",
      "Epoch 00048: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.2336 - accuracy: 0.9226 - val_loss: 1.7210 - val_accuracy: 0.7019\n",
      "Epoch 49/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2252 - accuracy: 0.9271\n",
      "Epoch 00049: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.2349 - accuracy: 0.9226 - val_loss: 1.9053 - val_accuracy: 0.6731\n",
      "Epoch 50/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2932 - accuracy: 0.8958\n",
      "Epoch 00050: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.2926 - accuracy: 0.8968 - val_loss: 1.8574 - val_accuracy: 0.7019\n",
      "Epoch 51/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2725 - accuracy: 0.9097\n",
      "Epoch 00051: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 265us/sample - loss: 0.2661 - accuracy: 0.9129 - val_loss: 1.7873 - val_accuracy: 0.6635\n",
      "Epoch 52/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2278 - accuracy: 0.9271\n",
      "Epoch 00052: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 265us/sample - loss: 0.2306 - accuracy: 0.9258 - val_loss: 1.6331 - val_accuracy: 0.7019\n",
      "Epoch 53/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2273 - accuracy: 0.9167\n",
      "Epoch 00053: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.2516 - accuracy: 0.9065 - val_loss: 1.7079 - val_accuracy: 0.6923\n",
      "Epoch 54/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1334 - accuracy: 0.9688\n",
      "Epoch 00054: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 255us/sample - loss: 0.2513 - accuracy: 0.9032 - val_loss: 1.8280 - val_accuracy: 0.7019\n",
      "Epoch 55/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2170 - accuracy: 0.9340\n",
      "Epoch 00055: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.2175 - accuracy: 0.9355 - val_loss: 1.7314 - val_accuracy: 0.7115\n",
      "Epoch 56/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1876 - accuracy: 0.9444\n",
      "Epoch 00056: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1830 - accuracy: 0.9452 - val_loss: 1.8716 - val_accuracy: 0.7404\n",
      "Epoch 57/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2170 - accuracy: 0.9271\n",
      "Epoch 00057: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.2231 - accuracy: 0.9226 - val_loss: 1.8302 - val_accuracy: 0.7212\n",
      "Epoch 58/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1828 - accuracy: 0.9306\n",
      "Epoch 00058: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.1894 - accuracy: 0.9290 - val_loss: 1.7247 - val_accuracy: 0.7212\n",
      "Epoch 59/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1694 - accuracy: 0.9479\n",
      "Epoch 00059: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.1658 - accuracy: 0.9516 - val_loss: 1.7135 - val_accuracy: 0.7212\n",
      "Epoch 60/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2274 - accuracy: 0.9132\n",
      "Epoch 00060: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.2250 - accuracy: 0.9129 - val_loss: 1.9694 - val_accuracy: 0.6827\n",
      "Epoch 61/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1831 - accuracy: 0.9444\n",
      "Epoch 00061: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1870 - accuracy: 0.9419 - val_loss: 2.0591 - val_accuracy: 0.6923\n",
      "Epoch 62/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2260 - accuracy: 0.9132\n",
      "Epoch 00062: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.2354 - accuracy: 0.9065 - val_loss: 1.8464 - val_accuracy: 0.7212\n",
      "Epoch 63/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2159 - accuracy: 0.9167\n",
      "Epoch 00063: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.2248 - accuracy: 0.9129 - val_loss: 1.8137 - val_accuracy: 0.7212\n",
      "Epoch 64/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.2098 - accuracy: 0.9375\n",
      "Epoch 00064: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.2105 - accuracy: 0.9194 - val_loss: 1.9197 - val_accuracy: 0.7019\n",
      "Epoch 65/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1897 - accuracy: 0.9297\n",
      "Epoch 00065: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1794 - accuracy: 0.9323 - val_loss: 1.9886 - val_accuracy: 0.7019\n",
      "Epoch 66/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1512 - accuracy: 0.9340\n",
      "Epoch 00066: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1434 - accuracy: 0.9387 - val_loss: 2.0567 - val_accuracy: 0.6923\n",
      "Epoch 67/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9132\n",
      "Epoch 00067: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.2087 - accuracy: 0.9129 - val_loss: 2.0621 - val_accuracy: 0.6923\n",
      "Epoch 68/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2133 - accuracy: 0.9132\n",
      "Epoch 00068: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.2034 - accuracy: 0.9194 - val_loss: 2.3436 - val_accuracy: 0.6827\n",
      "Epoch 69/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1971 - accuracy: 0.9258\n",
      "Epoch 00069: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.1817 - accuracy: 0.9355 - val_loss: 1.9265 - val_accuracy: 0.6923\n",
      "Epoch 70/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1393 - accuracy: 0.9583\n",
      "Epoch 00070: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1385 - accuracy: 0.9581 - val_loss: 2.0572 - val_accuracy: 0.7115\n",
      "Epoch 71/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1817 - accuracy: 0.9236\n",
      "Epoch 00071: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1871 - accuracy: 0.9194 - val_loss: 2.1286 - val_accuracy: 0.6923\n",
      "Epoch 72/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1640 - accuracy: 0.9514\n",
      "Epoch 00072: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1834 - accuracy: 0.9484 - val_loss: 2.3359 - val_accuracy: 0.6827\n",
      "Epoch 73/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1668 - accuracy: 0.9492\n",
      "Epoch 00073: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.1793 - accuracy: 0.9419 - val_loss: 1.9300 - val_accuracy: 0.6923\n",
      "Epoch 74/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.9414\n",
      "Epoch 00074: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1698 - accuracy: 0.9484 - val_loss: 1.9123 - val_accuracy: 0.6923\n",
      "Epoch 75/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1767 - accuracy: 0.9410\n",
      "Epoch 00075: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1922 - accuracy: 0.9355 - val_loss: 2.0684 - val_accuracy: 0.7308\n",
      "Epoch 76/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1424 - accuracy: 0.9453\n",
      "Epoch 00076: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 319us/sample - loss: 0.1500 - accuracy: 0.9419 - val_loss: 2.0034 - val_accuracy: 0.7115\n",
      "Epoch 77/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2206 - accuracy: 0.9201\n",
      "Epoch 00077: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.2167 - accuracy: 0.9226 - val_loss: 2.0804 - val_accuracy: 0.7019\n",
      "Epoch 78/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1477 - accuracy: 0.9648\n",
      "Epoch 00078: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.1621 - accuracy: 0.9484 - val_loss: 1.8289 - val_accuracy: 0.7404\n",
      "Epoch 79/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2079 - accuracy: 0.9201\n",
      "Epoch 00079: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 261us/sample - loss: 0.1996 - accuracy: 0.9226 - val_loss: 1.8221 - val_accuracy: 0.7308\n",
      "Epoch 80/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1679 - accuracy: 0.9062\n",
      "Epoch 00080: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 248us/sample - loss: 0.2253 - accuracy: 0.9161 - val_loss: 1.9418 - val_accuracy: 0.7212\n",
      "Epoch 81/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1256 - accuracy: 0.9688\n",
      "Epoch 00081: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 261us/sample - loss: 0.1758 - accuracy: 0.9484 - val_loss: 2.1640 - val_accuracy: 0.6923\n",
      "Epoch 82/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2186 - accuracy: 0.9062\n",
      "Epoch 00082: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.2255 - accuracy: 0.9065 - val_loss: 2.0485 - val_accuracy: 0.6923\n",
      "Epoch 83/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2549 - accuracy: 0.9236\n",
      "Epoch 00083: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.2507 - accuracy: 0.9226 - val_loss: 1.8871 - val_accuracy: 0.7212\n",
      "Epoch 84/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.2252 - accuracy: 0.9023\n",
      "Epoch 00084: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 297us/sample - loss: 0.2325 - accuracy: 0.9032 - val_loss: 1.8996 - val_accuracy: 0.7212\n",
      "Epoch 85/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1845 - accuracy: 0.9453\n",
      "Epoch 00085: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 300us/sample - loss: 0.1881 - accuracy: 0.9419 - val_loss: 1.9680 - val_accuracy: 0.6827\n",
      "Epoch 86/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1603 - accuracy: 0.9514\n",
      "Epoch 00086: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1546 - accuracy: 0.9516 - val_loss: 2.3256 - val_accuracy: 0.7115\n",
      "Epoch 87/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2361 - accuracy: 0.9097\n",
      "Epoch 00087: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.2400 - accuracy: 0.9065 - val_loss: 1.9493 - val_accuracy: 0.7308\n",
      "Epoch 88/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1904 - accuracy: 0.9336\n",
      "Epoch 00088: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.1949 - accuracy: 0.9355 - val_loss: 1.8938 - val_accuracy: 0.7404\n",
      "Epoch 89/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.2530 - accuracy: 0.8958\n",
      "Epoch 00089: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.2517 - accuracy: 0.8968 - val_loss: 1.8912 - val_accuracy: 0.7212\n",
      "Epoch 90/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1712 - accuracy: 0.9583\n",
      "Epoch 00090: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1817 - accuracy: 0.9548 - val_loss: 1.9337 - val_accuracy: 0.7019\n",
      "Epoch 91/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1344 - accuracy: 0.9618\n",
      "Epoch 00091: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 265us/sample - loss: 0.1309 - accuracy: 0.9613 - val_loss: 2.0360 - val_accuracy: 0.7019\n",
      "Epoch 92/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1549 - accuracy: 0.9453\n",
      "Epoch 00092: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1638 - accuracy: 0.9452 - val_loss: 1.9854 - val_accuracy: 0.7115\n",
      "Epoch 93/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1671 - accuracy: 0.9444\n",
      "Epoch 00093: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.1745 - accuracy: 0.9452 - val_loss: 1.9822 - val_accuracy: 0.7212\n",
      "Epoch 94/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1718 - accuracy: 0.9062\n",
      "Epoch 00094: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 258us/sample - loss: 0.1644 - accuracy: 0.9452 - val_loss: 1.9480 - val_accuracy: 0.7212\n",
      "Epoch 95/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1635 - accuracy: 0.9514\n",
      "Epoch 00095: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1533 - accuracy: 0.9548 - val_loss: 2.0996 - val_accuracy: 0.7019\n",
      "Epoch 96/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9479\n",
      "Epoch 00096: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1325 - accuracy: 0.9419 - val_loss: 2.0851 - val_accuracy: 0.7115\n",
      "Epoch 97/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1501 - accuracy: 0.9375\n",
      "Epoch 00097: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.1449 - accuracy: 0.9419 - val_loss: 2.1047 - val_accuracy: 0.7019\n",
      "Epoch 98/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1500 - accuracy: 0.9410\n",
      "Epoch 00098: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1458 - accuracy: 0.9452 - val_loss: 2.3263 - val_accuracy: 0.7212\n",
      "Epoch 99/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1142 - accuracy: 0.9688\n",
      "Epoch 00099: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1069 - accuracy: 0.9710 - val_loss: 2.0935 - val_accuracy: 0.7019\n",
      "Epoch 100/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1693 - accuracy: 0.9375\n",
      "Epoch 00100: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1653 - accuracy: 0.9387 - val_loss: 2.0848 - val_accuracy: 0.7308\n",
      "Epoch 101/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1180 - accuracy: 0.9549\n",
      "Epoch 00101: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1122 - accuracy: 0.9581 - val_loss: 2.4278 - val_accuracy: 0.6731\n",
      "Epoch 102/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1379 - accuracy: 0.9410\n",
      "Epoch 00102: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.1415 - accuracy: 0.9387 - val_loss: 2.1634 - val_accuracy: 0.7308\n",
      "Epoch 103/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1340 - accuracy: 0.9479\n",
      "Epoch 00103: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.1342 - accuracy: 0.9484 - val_loss: 1.9987 - val_accuracy: 0.7308\n",
      "Epoch 104/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1631 - accuracy: 0.9410\n",
      "Epoch 00104: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1522 - accuracy: 0.9452 - val_loss: 2.0994 - val_accuracy: 0.7212\n",
      "Epoch 105/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1212 - accuracy: 0.9688\n",
      "Epoch 00105: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 261us/sample - loss: 0.1470 - accuracy: 0.9516 - val_loss: 2.1918 - val_accuracy: 0.7212\n",
      "Epoch 106/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1541 - accuracy: 0.9514\n",
      "Epoch 00106: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.1561 - accuracy: 0.9484 - val_loss: 2.2802 - val_accuracy: 0.7212\n",
      "Epoch 107/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.9375\n",
      "Epoch 00107: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.1531 - accuracy: 0.9387 - val_loss: 2.5264 - val_accuracy: 0.7308\n",
      "Epoch 108/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1207 - accuracy: 0.9653\n",
      "Epoch 00108: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 265us/sample - loss: 0.1258 - accuracy: 0.9613 - val_loss: 2.5150 - val_accuracy: 0.7308\n",
      "Epoch 109/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.1254 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 258us/sample - loss: 0.0947 - accuracy: 0.9645 - val_loss: 2.4270 - val_accuracy: 0.7212\n",
      "Epoch 110/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1334 - accuracy: 0.9583\n",
      "Epoch 00110: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 290us/sample - loss: 0.1281 - accuracy: 0.9613 - val_loss: 2.4625 - val_accuracy: 0.7115\n",
      "Epoch 111/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1475 - accuracy: 0.9375\n",
      "Epoch 00111: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.1571 - accuracy: 0.9355 - val_loss: 2.6671 - val_accuracy: 0.6827\n",
      "Epoch 112/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1274 - accuracy: 0.9514\n",
      "Epoch 00112: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 287us/sample - loss: 0.1374 - accuracy: 0.9419 - val_loss: 2.5802 - val_accuracy: 0.6923\n",
      "Epoch 113/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1569 - accuracy: 0.9444\n",
      "Epoch 00113: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1482 - accuracy: 0.9484 - val_loss: 2.5204 - val_accuracy: 0.6827\n",
      "Epoch 114/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1131 - accuracy: 0.9479\n",
      "Epoch 00114: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1132 - accuracy: 0.9484 - val_loss: 2.3948 - val_accuracy: 0.7019\n",
      "Epoch 115/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1339 - accuracy: 0.9549\n",
      "Epoch 00115: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.1361 - accuracy: 0.9548 - val_loss: 2.6273 - val_accuracy: 0.7115\n",
      "Epoch 116/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9549\n",
      "Epoch 00116: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1282 - accuracy: 0.9548 - val_loss: 2.6971 - val_accuracy: 0.7115\n",
      "Epoch 117/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1309 - accuracy: 0.9479\n",
      "Epoch 00117: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1301 - accuracy: 0.9516 - val_loss: 2.6656 - val_accuracy: 0.7115\n",
      "Epoch 118/130\n",
      "256/310 [=======================>......] - ETA: 0s - loss: 0.1231 - accuracy: 0.9492\n",
      "Epoch 00118: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 303us/sample - loss: 0.1215 - accuracy: 0.9516 - val_loss: 2.7910 - val_accuracy: 0.7019\n",
      "Epoch 119/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1180 - accuracy: 0.9653\n",
      "Epoch 00119: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 281us/sample - loss: 0.1181 - accuracy: 0.9645 - val_loss: 2.6314 - val_accuracy: 0.7115\n",
      "Epoch 120/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1136 - accuracy: 0.9618\n",
      "Epoch 00120: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1253 - accuracy: 0.9548 - val_loss: 2.6912 - val_accuracy: 0.7019\n",
      "Epoch 121/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1366 - accuracy: 0.9514\n",
      "Epoch 00121: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 277us/sample - loss: 0.1308 - accuracy: 0.9548 - val_loss: 2.8594 - val_accuracy: 0.6923\n",
      "Epoch 122/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1347 - accuracy: 0.9514\n",
      "Epoch 00122: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 271us/sample - loss: 0.1284 - accuracy: 0.9548 - val_loss: 3.0359 - val_accuracy: 0.6827\n",
      "Epoch 123/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1225 - accuracy: 0.9514\n",
      "Epoch 00123: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.1142 - accuracy: 0.9548 - val_loss: 2.8906 - val_accuracy: 0.6923\n",
      "Epoch 124/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.2664 - accuracy: 0.8438\n",
      "Epoch 00124: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 265us/sample - loss: 0.1333 - accuracy: 0.9516 - val_loss: 2.9267 - val_accuracy: 0.7019\n",
      "Epoch 125/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1036 - accuracy: 0.9618\n",
      "Epoch 00125: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.1041 - accuracy: 0.9613 - val_loss: 2.9309 - val_accuracy: 0.7019\n",
      "Epoch 126/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1347 - accuracy: 0.9583\n",
      "Epoch 00126: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.1433 - accuracy: 0.9548 - val_loss: 2.9076 - val_accuracy: 0.7019\n",
      "Epoch 127/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.0988 - accuracy: 0.9583\n",
      "Epoch 00127: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 274us/sample - loss: 0.0981 - accuracy: 0.9548 - val_loss: 2.7745 - val_accuracy: 0.7115\n",
      "Epoch 128/130\n",
      " 32/310 [==>...........................] - ETA: 0s - loss: 0.2424 - accuracy: 0.8750\n",
      "Epoch 00128: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 268us/sample - loss: 0.1255 - accuracy: 0.9419 - val_loss: 2.4779 - val_accuracy: 0.7019\n",
      "Epoch 129/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1148 - accuracy: 0.9549\n",
      "Epoch 00129: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 294us/sample - loss: 0.1122 - accuracy: 0.9548 - val_loss: 2.3443 - val_accuracy: 0.7212\n",
      "Epoch 130/130\n",
      "288/310 [==========================>...] - ETA: 0s - loss: 0.1304 - accuracy: 0.9514\n",
      "Epoch 00130: val_loss did not improve from 1.21957\n",
      "310/310 [==============================] - 0s 284us/sample - loss: 0.1509 - accuracy: 0.9419 - val_loss: 2.4513 - val_accuracy: 0.7404\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(x_train, y_train, epochs = 130, batch_size = 32, validation_split=0.25, verbose=1, callbacks=[checkpoint])\n",
    "#history = model.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data= (x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 16, 128)           53504     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                627       \n",
      "=================================================================\n",
      "Total params: 325,523\n",
      "Trainable params: 325,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 308us/sample - loss: 2.3510 - accuracy: 0.7692\n",
      "76.92307829856873\n"
     ]
    }
   ],
   "source": [
    "_,acc=model.evaluate(x_val,y_val)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    #print(test_word)            ##\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "    x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "    pred = model.predict_proba(x)\n",
    "  \n",
    "    return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-86-ec8984824710>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-86-ec8984824710>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    return pred\u001b[0m\n\u001b[1;37m               \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''''def predictions1(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "  \n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "    x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "    pred = model.predict_classes(x)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    " \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    pred_intent=classes[0]\n",
    " \n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "    return pred_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    " \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    pred_intent=classes[0]\n",
    " \n",
    "    #for i in range(pred.shape[1]):\n",
    "        #print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "    return pred_intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def word_for_id(integer, tokenizer):\n",
    "    for word, index in Tokenizer.word_index.items():\n",
    "        if index >=0:\n",
    "            return word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext=\"need some help?\"\\npred = predictions(text)\\n\\n#predictions(text)\\nget_final_output(pred, unique_intent)\\n#word = word_for_id(pred, output_tokenizer)\\n\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "text=\"need some help?\"\n",
    "pred = predictions(text)\n",
    "\n",
    "#predictions(text)\n",
    "get_final_output(pred, unique_intent)\n",
    "#word = word_for_id(pred, output_tokenizer)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_GQ_help = [\"Yes, I can help you with anything.\", \"What do you need help with?\", \"Sure. What can I help you with?\"]\n",
    "A_FAQ_why_reg = [\"Because there's no other event like this\", \"you will learn many new things\", \n",
    "             \" No other event which has both conference and hackathon\"]\n",
    "A_SQ_IEEE = [\"IEEE-VIT is one of the most active and prestigious chapters of VIT\", \"IEEE-VIT is a student technical chapter\"\n",
    "             \"IEEE-VIT is a student based chapter which falls under region 10\"]\n",
    "A_SQ_reg_fee = [\"No, the event is free to attend\", \"There is no registration fee for the event\", \"No, the registrations are free\"]\n",
    "A_GQ_name = [\"My name is IEEE bot\", \" I am IEEE bot\", \"You are talking to IEEE bot\"]\n",
    "A_FAQ_food = [\"Yes, food will be provided\",\"Yes, refreshments will be provided\",\"Sure, everyone needs food\"]\n",
    "A_GQ_query =[ \"Yes, go ahead\",\"Ask me any queries you have\",\"go ahead, ask away\"]\n",
    "A_GQ_bot = [\"That's right, I am a chatbot\", \"Yes, I am a chatbot\", \"I am a bot. Chatbot\"]\n",
    "A_SQ_event_details = [\"This event is all about learning\", \"It's a hackathon\",\"It's a fun event for sure\"]\n",
    "A_GQ_gen = [\"I am good\",\"I am doing great\",\"Never better\"]\n",
    "A_SQ_event_prize = [\"Yes, there will be prizes\",\"Definitely\",\"Yes there will be, along with goodies\"]\n",
    "A_FAQ_contact_info = [\"Please contact us via insta\",\"We are reachable from our insta handle\",\"You can contact us anytime via insta\"]\n",
    "A_JOIN = [\"Please contact us via collaborations.ieeevit@gmail.com, thank you.\"]\n",
    "A_SQ_event_speakers=[\"speaker 1 and 2 will join us, stay tuned for more\",\" We have confirmed speaker 1 and 2\"]\n",
    "A_SQ_event_date=[\"From 10th to 12th\",\"It will be for 2 days starting from 10th\",\"10th-12th\"]\n",
    "A_SQ_reg_lastdate=[\"The last day to register is 9th\",\"You can register by 9th\",\"9th is the last day to register\"]\n",
    "A_SQ_event_schedule=[\"There will be talks followed by a hackthon\",\"First talks then hack\",\"Hack after speech\"]\n",
    "A_FAQ_accom=[\"No, accomodation can't be provided\",\"Sorry, we dont provide accomodation\",\"There is no accomodation facility from our side\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def F1():\n",
    "    print('bot: ',random.choice(A_GQ_help))\n",
    "def F2():\n",
    "    print('bot: ',random.choice(A_FAQ_why_reg))\n",
    "def F3():\n",
    "    print('bot: ',random.choice(A_SQ_IEEE))\n",
    "def F4():\n",
    "    print('bot: ',random.choice(A_SQ_reg_fee))\n",
    "def F5():\n",
    "    print('bot: ',random.choice(A_GQ_name))\n",
    "def F6():\n",
    "    print('bot: ',random.choice(A_FAQ_food))\n",
    "def F7():\n",
    "    print('bot: ',random.choice(A_GQ_query))\n",
    "def F8():\n",
    "    print('bot: ',random.choice(A_GQ_bot))\n",
    "def F9():\n",
    "    print('bot: ',random.choice(A_SQ_event_details))\n",
    "def F10():\n",
    "    print('bot: ',random.choice(A_GQ_gen))\n",
    "def F11():\n",
    "    print('bot: ',random.choice(A_SQ_event_prize))\n",
    "def F12():\n",
    "    print('bot: ',random.choice(A_FAQ_contact_info))\n",
    "def F13():\n",
    "    print('bot: ',random.choice(A_JOIN))\n",
    "def F14():\n",
    "    print('bot: ',random.choice(A_SQ_event_speakers))\n",
    "def F15():\n",
    "    print('bot: ',random.choice(A_SQ_event_date))\n",
    "def F16():\n",
    "    print('bot: ',random.choice(A_SQ_reg_lastdate))\n",
    "def F17():\n",
    "    print('bot: ',random.choice(A_SQ_event_schedule))\n",
    "def F18():\n",
    "    print('bot: ',random.choice(A_FAQ_accom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_output():\n",
    "    for i in range(100):\n",
    "        text=input('\\nYou: ')\n",
    "        if text == 'quit':\n",
    "            print('bye')\n",
    "            break\n",
    "        else : \n",
    "            pred = predictions(text)\n",
    "            if get_final_output(pred, unique_intent) == 'GQ.help':\n",
    "                F1()\n",
    "            elif get_final_output(pred, unique_intent) == 'FAQ.why_reg':\n",
    "                F2()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.IEEE':\n",
    "                F3()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.reg_fee':\n",
    "                F4()\n",
    "            elif get_final_output(pred, unique_intent) == 'GQ.name':\n",
    "                F5()\n",
    "            elif get_final_output(pred, unique_intent) == 'FAQ.food':\n",
    "                F6()\n",
    "            elif get_final_output(pred, unique_intent) == 'GQ.query':\n",
    "                F7()\n",
    "            elif get_final_output(pred, unique_intent) == 'GQ.bot':\n",
    "                F8()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.event_details':\n",
    "                F9()\n",
    "            elif get_final_output(pred, unique_intent) == 'GQ.gen':\n",
    "                F10()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.event_prize':\n",
    "                F11()\n",
    "            elif get_final_output(pred, unique_intent) == 'FAQ.contact_info':\n",
    "                F12()\n",
    "            elif get_final_output(pred, unique_intent) == 'JOIN.speaker':\n",
    "                F13()\n",
    "            elif get_final_output(pred, unique_intent) == 'JOIN.sponsor':\n",
    "                F13()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.event_speakers':\n",
    "                F14()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.event_date':\n",
    "                F15()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.reg_lastdate':\n",
    "                F16()\n",
    "            elif get_final_output(pred, unique_intent) == 'SQ.event_schedule':\n",
    "                F17()\n",
    "            elif get_final_output(pred, unique_intent) == 'FAQ.accom':\n",
    "                F18()\n",
    "            else:\n",
    "                print(\"Please enter a valid response\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You: what is your name?\n",
      "bot:  My name is IEEE bot\n",
      "\n",
      "You: nice to meet you\n",
      "bot:  Never better\n",
      "\n",
      "You: can you help me?\n",
      "bot:  Yes, I can help you with anything.\n",
      "\n",
      "You: will we get food?\n",
      "bot:  Sure, everyone needs food\n",
      "\n",
      "You: I want to be a sponsor\n",
      "bot:  Please contact us via collaborations.ieeevit@gmail.com, thank you.\n",
      "\n",
      "You: quit\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "user_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dlmodel] *",
   "language": "python",
   "name": "conda-env-.conda-dlmodel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
